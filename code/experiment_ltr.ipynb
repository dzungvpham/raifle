{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from attack import (\n",
    "    reconstruct_interactions,\n",
    ")\n",
    "from dataset import (\n",
    "    LearningToRankDataset,\n",
    ")\n",
    "from more_itertools import grouper\n",
    "from ranker import (\n",
    "    LinearPDGDRanker,\n",
    "    Neural1LayerPDGDRanker,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import (\n",
    "    CascadeClickModel,\n",
    "    Metrics,\n",
    "    apply_gaussian_mechanism,\n",
    "    LtrEvaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=2023):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Change the dataset here\n",
    "# Possible values: MQ2007, MSLR-WEB10K.\n",
    "# (MQ2008 is also possible but it's a smaller dataset than MQ2007 and the results are very similar.)\n",
    "dataset = \"MQ2007\"\n",
    "\n",
    "if dataset == \"MSLR-WEB10K\":\n",
    "    data = LearningToRankDataset(\"../dataset/MSLR-WEB10K/Fold1/train.txt\", normalize=True)\n",
    "    click_models = {\n",
    "        \"navigational\": CascadeClickModel(prob_click=[0.05, 0.3, 0.5, 0.7, 0.95], prob_stop=[0.2, 0.3, 0.5, 0.7, 0.9]),\n",
    "        \"informational\": CascadeClickModel(prob_click=[0.4, 0.6, 0.7, 0.8, 0.9], prob_stop=[0.1, 0.2, 0.3, 0.4, 0.5]),\n",
    "    }\n",
    "    # num_query_per_user = [12, 24, 48]\n",
    "    num_query_per_user = [48] # Scaled down for artifact eval\n",
    "else:\n",
    "    data = LearningToRankDataset(f\"../dataset/{dataset}/Fold1/train.txt\", normalize=False)\n",
    "    click_models = {\n",
    "        \"navigational\": CascadeClickModel(prob_click=[0.05, 0.5, 0.95], prob_stop=[0.2, 0.5, 0.9]),\n",
    "        \"informational\": CascadeClickModel(prob_click=[0.4, 0.7, 0.9], prob_stop=[0.1, 0.3, 0.5]),\n",
    "    }\n",
    "    # num_query_per_user = [4, 8, 16]\n",
    "    num_query_per_user = [16] # Scaled down for artifact eval\n",
    "\n",
    "num_features = data.get_num_features()\n",
    "models = {\n",
    "    \"linear_pdgd\": LinearPDGDRanker(num_features),\n",
    "    # \"neural_4_pdgd\": Neural1LayerPDGDRanker(num_features, hidden_size=4),\n",
    "    # \"neural_8_pdgd\": Neural1LayerPDGDRanker(num_features, hidden_size=8),\n",
    "    \"neural_16_pdgd\": Neural1LayerPDGDRanker(num_features, hidden_size=16),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main results in Table VII of Section VI.B as well as DP results in Table IX of Section VII.A\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Local learning parameters\n",
    "num_item_per_ranking = 10\n",
    "local_lr = 1e-01\n",
    "num_sim_round = 1\n",
    "\n",
    "# Stealthiness parameter: Control % of features to manipulate. Selection is random.\n",
    "# alpha=1.0 means all features will be manipulated, 0.0 means no manipulation\n",
    "# alphas = [0.0, 0.5, 0.75, 1.0]\n",
    "alphas = [0.0, 1.0]\n",
    "\n",
    "# Reconstruction attack parameters\n",
    "num_atk = 1\n",
    "max_iter = 1000\n",
    "atk_lr = 0.1\n",
    "\n",
    "# Differential privacy parameters\n",
    "epsilons = [1.0, 20.0, 100.0, 500.0, math.inf]\n",
    "delta = 1e-08\n",
    "sensitivity = 0.5\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "# Local training algorithm\n",
    "def train(model, params, grouped_train_data, local_lr=local_lr):\n",
    "    cur_params = params.clone()\n",
    "\n",
    "    for features, ranking, interactions in grouped_train_data:\n",
    "        cur_grad = model.grad(\n",
    "            cur_params,\n",
    "            features,\n",
    "            ranking,\n",
    "            interactions,\n",
    "        )\n",
    "        cur_params = cur_params + local_lr * cur_grad\n",
    "\n",
    "    return cur_params - params\n",
    "\n",
    "def simulate_attack(model, model_name, grouped_data, click_model, epsilons, click_model_name, num_query):\n",
    "    params = model.gen_params()\n",
    "    indices = []\n",
    "    start_ind = 0\n",
    "    grouped_train_data_dict = {\n",
    "        alpha: [] for alpha in alphas\n",
    "    }\n",
    "\n",
    "    # Sample clicks and prepare manipulated data\n",
    "    for relevances, features in grouped_data:\n",
    "        if len(relevances) == 1:\n",
    "            continue\n",
    "        features = torch.Tensor(features)\n",
    "        ranking = model.rank(params, features, sample=True)[:num_item_per_ranking]\n",
    "        interactions = torch.Tensor(click_model.click(ranking, relevances)) # Ground truth\n",
    "        \n",
    "        features = features[ranking]\n",
    "        # Remap the original ranking into the correct range\n",
    "        _, ranking = torch.where(\n",
    "            torch.sort(ranking)[0].unsqueeze(1) == ranking.unsqueeze(0)\n",
    "        )\n",
    "        num_items = len(ranking)        \n",
    "        noise = torch.normal(0.0, 0.1, features.shape)\n",
    "        for alpha in alphas:\n",
    "            mask = torch.ones_like(features)\n",
    "            selected_features = random.sample(list(range(num_features)), int(num_features * alpha))\n",
    "            mask[:, selected_features] = 0.0\n",
    "            features_adm = mask * features + (1.0 - mask) * noise\n",
    "            grouped_train_data_dict[alpha].append((features_adm, ranking, interactions))\n",
    "        \n",
    "        indices.append((start_ind, start_ind + num_items))\n",
    "        start_ind += num_items\n",
    "\n",
    "    if len(grouped_train_data_dict[alphas[0]]) < 1:\n",
    "        return\n",
    "    \n",
    "    # Local training\n",
    "    raw_target_dict = {\n",
    "        key: train(\n",
    "            model,\n",
    "            params,\n",
    "            random.sample(train_data, len(train_data)),\n",
    "            local_lr,\n",
    "        ) for key, train_data in grouped_train_data_dict.items()\n",
    "    }\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "        for key, raw_target in raw_target_dict.items():\n",
    "            target = apply_gaussian_mechanism(raw_target, epsilon, delta, sensitivity)\n",
    "            # Reconstruct\n",
    "            train_data = grouped_train_data_dict[key]\n",
    "            preds_raw, _ = reconstruct_interactions(\n",
    "                lambda I: (train(\n",
    "                    model,\n",
    "                    params,\n",
    "                    [\n",
    "                        (features, ranking, I[indices[idx][0] : indices[idx][1]])\n",
    "                        for idx, (features, ranking, _) in enumerate(train_data)\n",
    "                    ],\n",
    "                    local_lr,\n",
    "                )) / local_lr,\n",
    "                target / local_lr,\n",
    "                indices[-1][1],\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "            interactions = torch.cat([I for (_, _, I) in train_data]) # Ground truth\n",
    "\n",
    "            metrics.update(\n",
    "                f\"{model_name}_{click_model_name}_{num_query}_query_eps_{epsilon}_{key}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "for _ in tqdm(range(num_sim_round)):\n",
    "    query_ids = data.get_all_query_ids()\n",
    "    query_ids = random.sample(query_ids, len(query_ids))\n",
    "\n",
    "    for num_query in num_query_per_user:\n",
    "        print(\"Num query\", num_query)\n",
    "        for qids in tqdm(grouper(query_ids, num_query, incomplete=\"ignore\"), total=len(query_ids)//num_query):\n",
    "            grouped_data = data.get_data_for_queries(list(qids))\n",
    "            for model_name, model in models.items():\n",
    "                for click_model_name, click_model in click_models.items():\n",
    "                    simulate_attack(model, model_name, grouped_data, click_model, epsilons, click_model_name, num_query)\n",
    "\n",
    "metrics.print_summary()\n",
    "metrics.save(\"../output/ltr_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility (NDCG) vs DP and number of users attacked\n",
    "set_seed()\n",
    "\n",
    "test_data = LearningToRankDataset(f\"../dataset/{dataset}/Fold1/test.txt\", normalize=True)\n",
    "\n",
    "num_rounds = 10\n",
    "num_query_per_user = [1]\n",
    "num_item_per_ranking = 10\n",
    "local_lr = 1e-01\n",
    "epsilons = [1.0, 20.0, 100.0, 500.0, math.inf]\n",
    "delta = 1e-08\n",
    "sensitivity = 0.5\n",
    "num_users_per_agg = 100\n",
    "num_attack_users = [0, 5, 10, 20, 40, 60, 80, 90, 95, 100]\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"model_name\": [],\n",
    "    \"click_model\": [],\n",
    "    \"epsilon\": [],\n",
    "    \"n_attacked_users\": [],\n",
    "    \"ndcg\": [],    \n",
    "})\n",
    "evaluator = LtrEvaluator(test_data, num_item_per_ranking)\n",
    "\n",
    "query_ids = data.get_all_query_ids()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    for click_model_name, click_model in click_models.items():\n",
    "        orig_model_params = model.gen_params()\n",
    "        for n_users in num_attack_users:\n",
    "            for epsilon in epsilons:\n",
    "                print(f\"Model: {model_name} | Click model: {click_model_name} | Epsilon: {epsilon} | Num attacked users: {n_users}\")\n",
    "                model_params = torch.clone(orig_model_params)\n",
    "                ndcgs = []\n",
    "                ndcgs.append(evaluator.calculate_average_offline_ndcg(model, model_params))\n",
    "\n",
    "                for _ in tqdm(range(num_rounds)):\n",
    "                    query_ids = random.sample(query_ids, len(query_ids))\n",
    "                    grad_arr = []\n",
    "                    n_users_to_attack = n_users\n",
    "\n",
    "                    for qid in query_ids:\n",
    "                        relevances, features = data.get_data_for_queries([qid])[0]\n",
    "\n",
    "                        features = torch.Tensor(features)\n",
    "                        ranking = model.rank(model_params, features, sample=True)[:num_item_per_ranking]\n",
    "                        clicks = click_model.click(ranking, relevances, filter_all_or_zero=False)\n",
    "                        if not np.any(clicks) or np.all(clicks):\n",
    "                            continue\n",
    "                        interactions = torch.Tensor(clicks)\n",
    "                        features = features[ranking]\n",
    "\n",
    "                        if n_users_to_attack > 0:\n",
    "                            features = torch.normal(0, 0.1, features.shape)\n",
    "                            n_users_to_attack -= 1\n",
    "\n",
    "                        # Remap the original ranking into the correct range\n",
    "                        _, ranking = torch.where(\n",
    "                            torch.sort(ranking)[0].unsqueeze(1) == ranking.unsqueeze(0)\n",
    "                        )\n",
    "                        \n",
    "                        raw_grad = local_lr * model.grad(\n",
    "                            model_params,\n",
    "                            features,\n",
    "                            ranking,\n",
    "                            interactions,\n",
    "                        )\n",
    "\n",
    "                        grad_arr.append(apply_gaussian_mechanism(raw_grad, epsilon, delta, sensitivity))\n",
    "\n",
    "                        if (len(grad_arr) == num_users_per_agg):\n",
    "                            model_params = model_params + torch.stack(grad_arr).mean(dim=0)\n",
    "                            grad_arr = []\n",
    "                            n_users_to_attack = n_users\n",
    "\n",
    "                    if (len(grad_arr) > 0):\n",
    "                        model_params = model_params + torch.stack(grad_arr).mean(dim=0)\n",
    "                    \n",
    "                    ndcgs.append(evaluator.calculate_average_offline_ndcg(model, model_params))\n",
    "                    \n",
    "                results.loc[len(results.index), :] = {\n",
    "                    \"model_name\": model_name,\n",
    "                    \"click_model\": click_model_name,\n",
    "                    \"epsilon\": epsilon,\n",
    "                    \"n_attacked_users\": n_users,\n",
    "                    \"ndcg\": ndcgs[-1]\n",
    "                }\n",
    "\n",
    "                print(f\"NDCG: {ndcgs[-1]}\")\n",
    "\n",
    "print(results.groupby([\"model_name\", \"click_model\", \"epsilon\", \"n_attacked_users\"]).describe().to_string())\n",
    "results.to_csv(\"../output/ltr_utility.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secure Aggregration + LDP (Table XI of Section VII.B)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Local training parameters\n",
    "num_query_per_user = [4]\n",
    "num_item_per_ranking = 10\n",
    "local_lr = 1e-01\n",
    "num_sim_round = 1\n",
    "\n",
    "# Reconstruction attack parameters\n",
    "num_atk = 1\n",
    "max_iter = 1000\n",
    "atk_lr = 0.1\n",
    "\n",
    "num_users = [10, 100, 500, 1000]\n",
    "epsilons = [700.0, 500.0, 300.0, 100.0]\n",
    "delta = 1e-08\n",
    "sensitivity = 0.5\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "def train(model, params, grouped_train_data, local_lr=local_lr):\n",
    "    cur_params = params.clone()\n",
    "\n",
    "    for features, ranking, interactions in grouped_train_data:\n",
    "        cur_grad = model.grad(\n",
    "            cur_params,\n",
    "            features,\n",
    "            ranking,\n",
    "            interactions,\n",
    "        )\n",
    "\n",
    "        cur_params = cur_params + local_lr * cur_grad\n",
    "\n",
    "    return cur_params - params\n",
    "\n",
    "def simulate_attack(model, model_name, grouped_data, click_model, epsilons, click_model_name, num_query):\n",
    "    params = model.gen_params()\n",
    "    indices = []\n",
    "    start_ind = 0\n",
    "    grouped_train_data_dict = {\n",
    "        \"sa\": []\n",
    "    }\n",
    "\n",
    "    for relevances, features in grouped_data:\n",
    "        if len(relevances) == 1:\n",
    "            continue\n",
    "        features = torch.Tensor(features)\n",
    "        ranking = model.rank(params, features, sample=True)[:num_item_per_ranking]\n",
    "        interactions = torch.Tensor(click_model.click(ranking, relevances))\n",
    "        \n",
    "        features = features[ranking]\n",
    "        # Remap the original ranking into the correct range\n",
    "        _, ranking = torch.where(\n",
    "            torch.sort(ranking)[0].unsqueeze(1) == ranking.unsqueeze(0)\n",
    "        )\n",
    "        num_items = len(ranking)\n",
    "        features_adm = torch.normal(0.0, 0.1, features.shape)\n",
    "        grouped_train_data_dict[\"sa\"].append((features_adm, ranking, interactions))\n",
    "        \n",
    "        indices.append((start_ind, start_ind + num_items))\n",
    "        start_ind += num_items\n",
    "\n",
    "    raw_target_dict = {\n",
    "        key: train(\n",
    "            model,\n",
    "            params,\n",
    "            random.sample(train_data, len(train_data)),\n",
    "            local_lr,\n",
    "        ) for key, train_data in grouped_train_data_dict.items()\n",
    "    }\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "        for num_user in num_users:\n",
    "            for key, raw_target in raw_target_dict.items():\n",
    "                target = apply_gaussian_mechanism(raw_target, epsilon, delta, sensitivity)\n",
    "                zeros = torch.zeros_like(target)\n",
    "                for _ in range(num_user - 1):\n",
    "                    target = target + apply_gaussian_mechanism(zeros, epsilon, delta, sensitivity)\n",
    "\n",
    "                train_data = grouped_train_data_dict[key]\n",
    "                preds_raw, _ = reconstruct_interactions(\n",
    "                    lambda I: (train(\n",
    "                        model,\n",
    "                        params,\n",
    "                        [\n",
    "                            (features, ranking, I[indices[idx][0] : indices[idx][1]])\n",
    "                            for idx, (features, ranking, _) in enumerate(train_data)\n",
    "                        ],\n",
    "                        local_lr,\n",
    "                    )) / local_lr,\n",
    "                    target / local_lr,\n",
    "                    indices[-1][1],\n",
    "                    lr=atk_lr,\n",
    "                    max_iter=max_iter,\n",
    "                    num_rounds=num_atk,\n",
    "                    return_raw=True,\n",
    "                )\n",
    "                preds = preds_raw.sigmoid().round().long()\n",
    "                interactions = torch.cat([I for (_, _, I) in train_data])\n",
    "\n",
    "                metrics.update(\n",
    "                    f\"{model_name}_{click_model_name}_{num_query}_query_eps_{epsilon}_{num_user}_users_{key}\",\n",
    "                    interactions,\n",
    "                    preds,\n",
    "                    preds_raw=preds_raw,\n",
    "                )\n",
    "\n",
    "for _ in tqdm(range(num_sim_round)):\n",
    "    query_ids = data.get_all_query_ids()\n",
    "    query_ids = random.sample(query_ids, len(query_ids))\n",
    "\n",
    "    for num_query in num_query_per_user:\n",
    "        print(\"Num query\", num_query)\n",
    "        for qids in tqdm(grouper(query_ids, num_query, incomplete=\"ignore\"), total=len(query_ids)//num_query):\n",
    "            grouped_data = data.get_data_for_queries(list(qids))\n",
    "            for model_name, model in models.items():\n",
    "                for click_model_name, click_model in click_models.items():                    \n",
    "                    simulate_attack(model, model_name, grouped_data, click_model, epsilons, click_model_name, num_query)\n",
    "\n",
    "metrics.print_summary()\n",
    "metrics.save(\"../output/ltr_sec_agg_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient visualization with t-sne\n",
    "set_seed()\n",
    "\n",
    "num_query_per_user = 12\n",
    "num_item_per_ranking = 10\n",
    "local_lr = 1e-01\n",
    "num_sim_round = 1000\n",
    "alphas = [0, 1]\n",
    "\n",
    "raw_target_dict = {\n",
    "    alpha: [] for alpha in alphas\n",
    "}\n",
    "\n",
    "def train(model, params, grouped_train_data, local_lr=local_lr):\n",
    "    cur_params = params.clone()\n",
    "\n",
    "    for features, ranking, interactions in grouped_train_data:\n",
    "        cur_grad = model.grad(\n",
    "            cur_params,\n",
    "            features,\n",
    "            ranking,\n",
    "            interactions,\n",
    "        )\n",
    "\n",
    "        cur_params = cur_params + local_lr * cur_grad\n",
    "\n",
    "    return cur_params - params\n",
    "\n",
    "def simulate_attack(model, model_name, grouped_data, click_model, click_model_name, num_query):\n",
    "    params = model.gen_params()\n",
    "    indices = []\n",
    "    start_ind = 0\n",
    "    grouped_train_data_dict = {\n",
    "        alpha: [] for alpha in alphas\n",
    "    }\n",
    "\n",
    "    for relevances, features in grouped_data:\n",
    "        if len(relevances) == 1:\n",
    "            continue\n",
    "        features = torch.Tensor(features)\n",
    "        ranking = model.rank(params, features, sample=True)[:num_item_per_ranking]\n",
    "        interactions = torch.Tensor(click_model.click(ranking, relevances))\n",
    "        \n",
    "        features = features[ranking]\n",
    "        # Remap the original ranking into the correct range\n",
    "        _, ranking = torch.where(\n",
    "            torch.sort(ranking)[0].unsqueeze(1) == ranking.unsqueeze(0)\n",
    "        )\n",
    "        num_items = len(ranking)        \n",
    "        noise = torch.normal(0.0, 0.1, features.shape)\n",
    "        for alpha in alphas:\n",
    "            mask = torch.ones_like(features)\n",
    "            selected_features = random.sample(list(range(num_features)), int(num_features * alpha))\n",
    "            mask[:, selected_features] = 0.0\n",
    "            features_adm = mask * features + (1.0 - mask) * noise\n",
    "            grouped_train_data_dict[alpha].append((features_adm, ranking, interactions))\n",
    "        \n",
    "        indices.append((start_ind, start_ind + num_items))\n",
    "        start_ind += num_items\n",
    "\n",
    "    if len(grouped_train_data_dict[alphas[0]]) < 1:\n",
    "        return\n",
    "    \n",
    "    for key, train_data in grouped_train_data_dict.items():    \n",
    "        raw_target_dict[key].append(train(\n",
    "            model,\n",
    "            params,\n",
    "            random.sample(train_data, len(train_data)),\n",
    "            local_lr,\n",
    "        ))\n",
    "\n",
    "\n",
    "query_ids = data.get_all_query_ids()\n",
    "for _ in tqdm(range(num_sim_round)):\n",
    "    qids = random.sample(query_ids, num_query_per_user)\n",
    "    grouped_data = data.get_data_for_queries(list(qids))\n",
    "    for model_name, model in models.items():\n",
    "        for click_model_name, click_model in click_models.items():                    \n",
    "            simulate_attack(model, model_name, grouped_data, click_model, click_model_name, num_query)\n",
    "\n",
    "num_grads = len(raw_target_dict[0.0])\n",
    "grads = torch.vstack([\n",
    "    torch.stack(raw_target_dict[0.0]),\n",
    "    torch.stack(raw_target_dict[1.0]),\n",
    "]).numpy()\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "handles = []  # Collect legend handles\n",
    "labels = []   # Collect legend labels\n",
    "\n",
    "for i, perplexity in enumerate([50, 100, 200, 400]):\n",
    "    visualizer = TSNE(n_components=2, perplexity=perplexity)\n",
    "    results = visualizer.fit_transform(grads)\n",
    "\n",
    "    ax = axes[i]\n",
    "    orange_scatter = ax.scatter(results[:num_grads, 0], results[:num_grads, 1], c=\"orange\", label=\"No ADM\", alpha=1.0, marker='.')\n",
    "    blue_scatter = ax.scatter(results[num_grads:, 0], results[num_grads:, 1], c=\"blue\", label=\"ADM\", alpha=0.5, marker='.')\n",
    "    ax.set_title(f'Perplexity = {perplexity}')\n",
    "    ax.set_xlabel('Component 1')\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Component 2')\n",
    "        handles.append(orange_scatter)\n",
    "        handles.append(blue_scatter)\n",
    "        labels.append(\"No ADM\")\n",
    "        labels.append(\"ADM\")\n",
    "\n",
    "lgd = fig.legend(handles, labels, loc=\"lower center\", bbox_to_anchor=(0.5, -0.1), ncols=2)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"../plots/tsne_mslr10k.pdf\", bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
