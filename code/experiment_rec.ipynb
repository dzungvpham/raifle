{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functorch\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torchopt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "from attack import (\n",
    "    reconstruct_interactions,\n",
    "    interaction_mia_fedrec,\n",
    ")\n",
    "from dataset import (\n",
    "    MovieLens,\n",
    "    Steam200K,\n",
    ")\n",
    "from ranker import (\n",
    "    CollaborativeFilteringRecommender,\n",
    "    NeuralCollaborativeFilteringRecommender,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import (\n",
    "    Metrics,\n",
    "    apply_gaussian_mechanism,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", message='.*make_functional.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    torch.manual_seed(2023)\n",
    "    random.seed(2023)\n",
    "    np.random.seed(2023)\n",
    "\n",
    "# data = MovieLens(\"../dataset/ML-100K/u.data\")\n",
    "data = Steam200K(\"../dataset/STEAM-200K/steam-200k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative Filtering + DP\n",
    "\n",
    "set_seed()\n",
    "\n",
    "user_ids = data.get_all_user_ids()\n",
    "item_ids = data.get_all_item_ids()\n",
    "user_id_to_idx = {id: idx for idx, id in enumerate(user_ids)}\n",
    "item_id_to_idx = {id: idx for idx, id in enumerate(item_ids)}\n",
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "embedding_dim = 64\n",
    "neg_sample_ratio = 4\n",
    "\n",
    "num_sim_round = 1\n",
    "atk_lr = 1e-01\n",
    "max_iter = 1000\n",
    "num_atk = 1\n",
    "\n",
    "epsilons = [1.0, 10.0, 20.0, 100.0, 500.0, math.inf]\n",
    "delta = 1e-08\n",
    "\n",
    "local_epoch = 20\n",
    "local_lr = 0.001\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "def train_fncf_functional(model, user_embedding, item_embeddings, interactions, num_epoch, lr):\n",
    "    user_embedding.grad = None\n",
    "    item_embeddings.grad = None\n",
    "    func_model, model_params = functorch.make_functional(model)\n",
    "    opt_params = (user_embedding, item_embeddings, *model_params)\n",
    "    # use_accelerated_op=True would be faster but prevent reconstruction for some reasons. Bug?\n",
    "    # eps_root must be set\n",
    "    optimizer = torchopt.FuncOptimizer(torchopt.adam(lr=lr, eps_root=1e-08))\n",
    "    for _ in range(num_epoch):\n",
    "        preds = func_model(opt_params[2:], opt_params[0], opt_params[1])\n",
    "        loss = F.binary_cross_entropy(preds.view(-1), interactions)\n",
    "        opt_params = optimizer.step(loss, opt_params)\n",
    "    return item_embeddings - opt_params[1], opt_params[2:]\n",
    "\n",
    "for _ in tqdm(range(num_sim_round)):\n",
    "    user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "    item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "    # fcf = CollaborativeFilteringRecommender()\n",
    "    fncf = NeuralCollaborativeFilteringRecommender(embedding_dim, [128, 64, 32])\n",
    "\n",
    "    for user_id in tqdm(user_ids):\n",
    "        # Set up training data\n",
    "        interacted_items = data.get_item_ids_for_users([user_id])[0]\n",
    "        non_interacted_items = data.get_non_interacted_item_ids_for_users([user_id])[0]\n",
    "\n",
    "        num_pos = len(interacted_items)\n",
    "        sampled_non_interacted_items = random.sample(\n",
    "            non_interacted_items,\n",
    "            min(num_pos * neg_sample_ratio, len(non_interacted_items)),\n",
    "        )\n",
    "        num_neg = len(sampled_non_interacted_items)\n",
    "        num_data = num_pos + num_neg\n",
    "\n",
    "        user_embedding = (\n",
    "            user_embeddings(torch.LongTensor([user_id_to_idx[user_id]]))\n",
    "            .detach()\n",
    "            .view(-1)\n",
    "        )\n",
    "        item_embedding = item_embeddings(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    torch.LongTensor([item_id_to_idx[id] for id in interacted_items]),\n",
    "                    torch.LongTensor(\n",
    "                        [item_id_to_idx[id] for id in sampled_non_interacted_items]\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        ).detach()\n",
    "        user_embedding.requires_grad_()\n",
    "        item_embedding.requires_grad_()\n",
    "        interactions = torch.cat([torch.ones(num_pos), torch.zeros(num_neg)])\n",
    "        random_user_emb = torch.rand(embedding_dim, requires_grad=True)\n",
    "\n",
    "        for epsilon in epsilons:\n",
    "            # FNCF setup\n",
    "            target, target_model_params = train_fncf_functional(fncf, user_embedding, item_embedding, interactions, local_epoch, local_lr)\n",
    "            target = apply_gaussian_mechanism(target.detach(), epsilon, delta, sensitivity=1e-01)\n",
    "            mean_norm = torch.linalg.vector_norm(target, dim=1).mean()\n",
    "            norm_scale = max(torch.Tensor([1.0]), torch.Tensor([1e+02]) / mean_norm)\n",
    "            custom_loss = lambda e1, e2: F.pairwise_distance(e1, e2).mean() * norm_scale\n",
    "\n",
    "            # FNCF simple\n",
    "            preds_raw, _ = reconstruct_interactions(\n",
    "                lambda I: train_fncf_functional(fncf, random_user_emb, item_embedding, I, local_epoch, local_lr)[0] / local_lr,\n",
    "                target / local_lr,\n",
    "                num_data,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_fn=custom_loss,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_simple_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "            # # FNCF jointly estimate user embedding\n",
    "            # preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "            #     lambda I, U: train_fncf_functional(fncf, U, item_embedding, I, local_epoch, local_lr)[0] / local_lr,\n",
    "            #     target / local_lr,\n",
    "            #     num_data,\n",
    "            #     private_params_size=embedding_dim,\n",
    "            #     lr=atk_lr,\n",
    "            #     max_iter=max_iter,\n",
    "            #     num_rounds=num_atk,\n",
    "            #     loss_fn=custom_loss,\n",
    "            #     return_raw=True,\n",
    "            # )\n",
    "            # preds = preds_raw.sigmoid().round().long()\n",
    "            # embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "            # metrics.update(\n",
    "            #     f\"FNCF_joint_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "            #     interactions,\n",
    "            #     preds,\n",
    "            #     preds_raw=preds_raw,\n",
    "            #     extra_data={\"est_user_emb_err\": embedding_err},\n",
    "            # )\n",
    "\n",
    "            # # FNCF simple with neural net params\n",
    "            # target_model_params = torch.cat([p.detach().view(-1) for p in target_model_params])\n",
    "            # preds_raw, _ = reconstruct_interactions(\n",
    "            #     lambda I: train_fncf_functional(fncf, random_user_emb, item_embedding, I, local_epoch, local_lr),\n",
    "            #     (target, target_model_params),\n",
    "            #     num_data,\n",
    "            #     lr=atk_lr,\n",
    "            #     max_iter=max_iter,\n",
    "            #     num_rounds=num_atk,\n",
    "            #     loss_fn=lambda t1, t2: custom_loss(t1[0] / local_lr, t2[0] / local_lr) + F.mse_loss(torch.cat([p.view(-1) for p in t1[1]]), t2[1]),\n",
    "            #     return_raw=True,\n",
    "            # )\n",
    "            # preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            # metrics.update(\n",
    "            #     f\"FNCF_simple_model_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "            #     interactions,\n",
    "            #     preds,\n",
    "            #     preds_raw=preds_raw,\n",
    "            # )\n",
    "\n",
    "            # # FNCF jointly estimate user embedding with neural net params\n",
    "            # preds_raw, _ = reconstruct_interactions(\n",
    "            #     lambda I, U: train_fncf_functional(fncf, U, item_embedding, I, local_epoch, local_lr),\n",
    "            #     (target, target_model_params),\n",
    "            #     num_data,\n",
    "            #     private_params_size=embedding_dim,\n",
    "            #     lr=atk_lr,\n",
    "            #     max_iter=max_iter,\n",
    "            #     num_rounds=num_atk,\n",
    "            #     loss_fn=lambda t1, t2: custom_loss(t1[0] / local_lr, t2[0] / local_lr) + F.mse_loss(torch.cat([p.view(-1) for p in t1[1]]), t2[1]),\n",
    "            #     return_raw=True,\n",
    "            # )\n",
    "            # preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            # metrics.update(\n",
    "            #     f\"FNCF_joint_model_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "            #     interactions,\n",
    "            #     preds,\n",
    "            #     preds_raw=preds_raw,\n",
    "            #     extra_data={\"est_user_emb_err\": embedding_err},\n",
    "            # )\n",
    "\n",
    "\n",
    "            # # FCF Simple\n",
    "            # sens = 0.005\n",
    "            # target = 0.01 * fcf.item_grad(user_embedding, item_embedding, interactions).detach()\n",
    "            # target = apply_gaussian_mechanism(target, epsilon, delta, sensitivity=sens)\n",
    "            # mean_norm = torch.linalg.vector_norm(target, dim=1).mean()\n",
    "            # norm_scale = max(torch.Tensor([1.0]), torch.Tensor([1e+02]) / mean_norm)\n",
    "            # custom_loss = lambda e1, e2: F.pairwise_distance(e1, e2).mean() * norm_scale\n",
    "\n",
    "            # preds_raw, _ = reconstruct_interactions(\n",
    "            #     lambda I: rescale_grad_for_dp(fcf.item_grad(random_user_emb, item_embedding, I, create_graph=True), epsilon, sens),\n",
    "            #     target,\n",
    "            #     num_data,\n",
    "            #     lr=atk_lr,\n",
    "            #     max_iter=max_iter,\n",
    "            #     num_rounds=num_atk,\n",
    "            #     loss_fn=custom_loss,\n",
    "            #     return_raw=True,\n",
    "            # )\n",
    "            # preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            # metrics.update(\n",
    "            #     f\"FCF_simple_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "            #     interactions,\n",
    "            #     preds,\n",
    "            #     preds_raw=preds_raw,\n",
    "            # )\n",
    "\n",
    "            # # FCF jointly estimate user embedding\n",
    "            # preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "            #     lambda I, U: 0.01 * fcf.item_grad(U, item_embedding, I, create_graph=True),\n",
    "            #     target,\n",
    "            #     num_data,\n",
    "            #     private_params_size=embedding_dim,\n",
    "            #     lr=atk_lr,\n",
    "            #     max_iter=max_iter,\n",
    "            #     num_rounds=num_atk,\n",
    "            #     loss_fn=custom_loss,\n",
    "            #     return_raw=True,\n",
    "            # )\n",
    "            # preds = preds_raw.sigmoid().round().long()\n",
    "            # embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "            # metrics.update(\n",
    "            #     f\"FCF_joint_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "            #     interactions,\n",
    "            #     preds.detach(),\n",
    "            #     preds_raw=preds_raw.detach(),\n",
    "            #     extra_data={\"est_user_emb_err\": embedding_err},\n",
    "            # )\n",
    "\n",
    "            # # Random guess\n",
    "            # preds_raw = torch.rand(num_data)\n",
    "            # metrics.update(\n",
    "            #     f\"Random_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "            #     interactions,\n",
    "            #     preds_raw.round().long(),\n",
    "            #     preds_raw=preds_raw,\n",
    "            # )\n",
    "\n",
    "            # IMIA FCF\n",
    "            # target = fcf.item_grad(user_embedding, item_embedding, interactions).detach()\n",
    "            # preds = interaction_mia_fedrec(\n",
    "            #     lambda I: fcf.item_grad(random_user_emb, item_embedding, I),\n",
    "            #     target,\n",
    "            #     num_data,\n",
    "            #     select_ratio=interactions.mean(),\n",
    "            # )\n",
    "\n",
    "            # metrics.update(\n",
    "            #     \"FCF_IMIA_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "            #     interactions,\n",
    "            #     preds,\n",
    "            # )\n",
    "\n",
    "# metrics.save(\"../output/rec_metrics.csv\")\n",
    "print(metrics.df[[\"name\", \"auc\", \"auc-pr\"]].groupby(\"name\").describe().to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
