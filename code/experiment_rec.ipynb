{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from attack import (\n",
    "    reconstruct_interactions,\n",
    "    interaction_mia_fedrec,\n",
    ")\n",
    "from dataset import (\n",
    "    MovieLens,\n",
    ")\n",
    "from more_itertools import grouper\n",
    "from ranker import (\n",
    "    CollaborativeFilteringRecommender,\n",
    "    NeuralCollaborativeFilteringRecommender,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import (\n",
    "    Metrics,\n",
    "    apply_gaussian_mechanism,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    torch.manual_seed(2023)\n",
    "    random.seed(2023)\n",
    "    np.random.seed(2023)\n",
    "\n",
    "data = MovieLens(\"../dataset/ML-100K/u.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation for collaborative filtering\n",
    "\n",
    "set_seed()\n",
    "\n",
    "num_sim_round = 10\n",
    "num_features = 64\n",
    "num_data = 1000\n",
    "atk_lr = 1e-01\n",
    "max_iter = 100000\n",
    "num_atk = 10\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "for _ in tqdm(range(num_sim_round)):\n",
    "    # features = torch.rand(num_data, num_features) * 2 - 1\n",
    "    # user_embedding = torch.rand(num_features) * 2 - 1\n",
    "    # user_embedding2 = torch.rand(num_features) * 2 - 1\n",
    "    features = torch.normal(0, 1, (num_data, num_features))\n",
    "    user_embedding = torch.normal(0, 1, (num_features,))\n",
    "    user_embedding2 = torch.normal(0, 1, (num_features,))\n",
    "\n",
    "    interactions = torch.randint(0, 2, (num_data,))\n",
    "    while interactions.sum() == 0:\n",
    "        interactions = torch.randint(0, 2, (num_data,))\n",
    "\n",
    "    preds_raw = torch.rand((num_data),)\n",
    "    metrics.update(\"Random\", interactions, preds_raw.round().long(), preds_raw=preds_raw)\n",
    "\n",
    "    ncf_rec = NeuralCollaborativeFilteringRecommender(num_features, [128, 64, 32])\n",
    "\n",
    "    target = ncf_rec.item_grad(user_embedding, features, interactions.float())\n",
    "    scale = max(1.0 / target.mean().abs(), 1.0)\n",
    "    target = scale * target\n",
    "\n",
    "    preds_raw, _ = reconstruct_interactions(\n",
    "        lambda I: scale * ncf_rec.item_grad(user_embedding2, features, I),\n",
    "        target,\n",
    "        num_data,\n",
    "        lr=atk_lr,\n",
    "        max_iter=max_iter,\n",
    "        num_rounds=num_atk,\n",
    "        return_raw=True,\n",
    "    )\n",
    "    preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "    metrics.update(\n",
    "        \"FedNCF_simple\",\n",
    "        interactions,\n",
    "        preds,\n",
    "        preds_raw=preds_raw,\n",
    "    )\n",
    "\n",
    "    target = ncf_rec.item_grad(user_embedding, features, interactions.float())\n",
    "    scale = max(1.0 / target.mean().abs(), 1.0)\n",
    "    target = scale * target\n",
    "\n",
    "    preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "        lambda I, U: scale * ncf_rec.item_grad(U, features, I),\n",
    "        target,\n",
    "        num_data,\n",
    "        private_params_size=num_features,\n",
    "        lr=atk_lr,\n",
    "        max_iter=max_iter,\n",
    "        num_rounds=num_atk,\n",
    "        return_raw=True,\n",
    "    )\n",
    "    preds = preds_raw.sigmoid().round().long()\n",
    "    \n",
    "    embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "    metrics.update(\n",
    "        \"FedNCF_private\",\n",
    "        interactions,\n",
    "        preds,\n",
    "        preds_raw=preds_raw,\n",
    "        extra_data={\"est_user_emb_err\": embedding_err},\n",
    "    )\n",
    "\n",
    "    item_grad = ncf_rec.item_grad(user_embedding, features, interactions.float()).flatten()\n",
    "    scale = max(1.0 / item_grad.mean().abs(), 1.0)\n",
    "\n",
    "    target = torch.cat(\n",
    "        [\n",
    "            scale * item_grad,\n",
    "            ncf_rec.feature_grad(user_embedding, features, interactions.float()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "        lambda I, U: torch.cat(\n",
    "            [\n",
    "                scale * ncf_rec.item_grad(U, features, I).flatten(),\n",
    "                ncf_rec.feature_grad(U, features, I, retain_graph=True),\n",
    "            ]\n",
    "        ),\n",
    "        target,\n",
    "        num_data,\n",
    "        private_params_size=num_features,\n",
    "        lr=atk_lr,\n",
    "        max_iter=max_iter,\n",
    "        num_rounds=num_atk,\n",
    "        return_raw=True,\n",
    "    )\n",
    "    preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "    embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "    metrics.update(\n",
    "        \"FedNCF_private2\",\n",
    "        interactions,\n",
    "        preds,\n",
    "        preds_raw=preds_raw,\n",
    "        extra_data={\"est_user_emb_err\": embedding_err},\n",
    "    )\n",
    "\n",
    "    target = ncf_rec.item_grad(user_embedding, features, interactions.float())\n",
    "\n",
    "    preds = interaction_mia_fedrec(\n",
    "        lambda I: ncf_rec.item_grad(user_embedding2, features, I.float()),\n",
    "        target,\n",
    "        num_data,\n",
    "        select_ratio=interactions.float().mean(),\n",
    "    )\n",
    "\n",
    "    metrics.update(\n",
    "        \"FedNCF_IMIA\",\n",
    "        interactions,\n",
    "        preds,\n",
    "    )\n",
    "\n",
    "print(metrics.df[[\"name\", \"auc\", \"auc-pr\"]].groupby(\"name\").describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative Filtering + DP\n",
    "\n",
    "set_seed()\n",
    "\n",
    "user_ids = data.get_all_user_ids()\n",
    "item_ids = data.get_all_item_ids()\n",
    "user_id_to_idx = {id: idx for idx, id in enumerate(user_ids)}\n",
    "item_id_to_idx = {id: idx for idx, id in enumerate(item_ids)}\n",
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "embedding_dim = 64\n",
    "neg_sample_ratio = 4\n",
    "\n",
    "num_sim_round = 1\n",
    "atk_lr = 1e-01\n",
    "max_iter = 1000\n",
    "num_atk = 5\n",
    "\n",
    "# epsilons = [1.0, 10.0, 100.0, math.inf]\n",
    "epsilons = [math.inf]\n",
    "delta = 1e-08\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "# def train_batch(model, user_embedding, item_embedding, interactions, num_batch, local_lr=0.1):\n",
    "#     item_embedding = item_embedding.clone()\n",
    "#     for _ in range(num_batch):\n",
    "#         item_grad = model.item_grad(user_embedding, item_embedding, interactions)\n",
    "#         item_embedding = item_embedding - local_lr * item_grad    \n",
    "#     return item_embedding\n",
    "\n",
    "for _ in tqdm(range(num_sim_round)):\n",
    "    user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "    item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "    fcf = CollaborativeFilteringRecommender()\n",
    "    fncf = NeuralCollaborativeFilteringRecommender(embedding_dim, [16, 8])\n",
    "\n",
    "    for user_id in tqdm(user_ids):\n",
    "        # Set up training data\n",
    "        interacted_items = data.get_item_ids_for_users([user_id])[0]\n",
    "        non_interacted_items = data.get_non_interacted_item_ids_for_users([user_id])[0]\n",
    "\n",
    "        num_pos = len(interacted_items)\n",
    "        sampled_non_interacted_items = random.sample(\n",
    "            non_interacted_items,\n",
    "            min(num_pos * neg_sample_ratio, len(non_interacted_items)),\n",
    "        )\n",
    "        num_neg = len(sampled_non_interacted_items)\n",
    "        num_data = num_pos + num_neg\n",
    "\n",
    "        user_embedding = (\n",
    "            user_embeddings(torch.LongTensor([user_id_to_idx[user_id]]))\n",
    "            .detach()\n",
    "            .view(-1)\n",
    "        )\n",
    "        item_embedding = item_embeddings(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    torch.LongTensor([item_id_to_idx[id] for id in interacted_items]),\n",
    "                    torch.LongTensor(\n",
    "                        [item_id_to_idx[id] for id in sampled_non_interacted_items]\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        ).detach()\n",
    "        item_embedding.requires_grad_()\n",
    "        interactions = torch.cat([torch.ones(num_pos), torch.zeros(num_neg)])\n",
    "        random_user_emb = torch.rand(embedding_dim)\n",
    "\n",
    "        for epsilon in epsilons:\n",
    "            # FCF Simple\n",
    "            target = fcf.item_grad(user_embedding, item_embedding, interactions)\n",
    "            target = apply_gaussian_mechanism(target, epsilon, delta, sensitivity=100)\n",
    "\n",
    "            preds_raw, _ = reconstruct_interactions(\n",
    "                lambda I: fcf.item_grad(random_user_emb, item_embedding, I),\n",
    "                target,\n",
    "                num_data,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FCF_simple_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "            # FCF jointly estimate user embedding\n",
    "            preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "                lambda I, U: fcf.item_grad(U, item_embedding, I),\n",
    "                target,\n",
    "                num_data,\n",
    "                private_params_size=embedding_dim,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "            embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FCF_joint_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "                extra_data={\"est_user_emb_err\": embedding_err},\n",
    "            )\n",
    "\n",
    "            # FNCF setup\n",
    "            target = fncf.item_grad(user_embedding, item_embedding, interactions)\n",
    "            target = apply_gaussian_mechanism(target, epsilon, delta, sensitivity=0.005)\n",
    "            mean_norm = torch.linalg.vector_norm(target, dim=1).mean()\n",
    "            norm_scale = max(torch.Tensor([1.0]), torch.Tensor([1e+02]) / mean_norm)\n",
    "            custom_loss = lambda e1, e2: F.pairwise_distance(e1, e2).mean() * norm_scale\n",
    "\n",
    "            # FNCF simple\n",
    "            preds_raw, _ = reconstruct_interactions(\n",
    "                lambda I: fncf.item_grad(random_user_emb, item_embedding, I, create_graph=True),\n",
    "                target,\n",
    "                num_data,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_func=custom_loss,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_simple_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "            # FNCF jointly estimate user embedding\n",
    "            preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "                lambda I, U: fncf.item_grad(U, item_embedding, I, create_graph=True),\n",
    "                target,\n",
    "                num_data,\n",
    "                private_params_size=embedding_dim,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_func=custom_loss,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "            embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_joint_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "                extra_data={\"est_user_emb_err\": embedding_err},\n",
    "            )\n",
    "\n",
    "            # # FNCF jointly estimate user embedding with neural net params\n",
    "            feature_grad = fncf.feature_grad(user_embedding, item_embedding, interactions)\n",
    "            feature_grad = apply_gaussian_mechanism(feature_grad, epsilon, delta, sensitivity=0.7)\n",
    "\n",
    "            preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "                lambda I, U: (\n",
    "                    fncf.item_grad(U, item_embedding, I, create_graph=True),\n",
    "                    fncf.feature_grad(U, item_embedding, I, create_graph=True),\n",
    "                ),\n",
    "                (target, feature_grad),\n",
    "                num_data,\n",
    "                private_params_size=embedding_dim,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_func=lambda t1, t2: custom_loss(t1[0], t2[0]) + F.mse_loss(t1[1], t2[1]),\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "            embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_joint_model_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "                extra_data={\"est_user_emb_err\": embedding_err},\n",
    "            )\n",
    "\n",
    "            # # FNCF simple with neural net params\n",
    "            preds_raw, _ = reconstruct_interactions(\n",
    "                lambda I: (\n",
    "                    fncf.item_grad(random_user_emb, item_embedding, I, create_graph=True),\n",
    "                    fncf.feature_grad(random_user_emb, item_embedding, I, create_graph=True),\n",
    "                ),\n",
    "                (target, feature_grad),\n",
    "                num_data,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_func=lambda t1, t2: custom_loss(t1[0], t2[0]) + F.mse_loss(t1[1], t2[1]),\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_simple_model_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "            # Random guess\n",
    "            preds_raw = torch.rand(num_data)\n",
    "            metrics.update(\n",
    "                f\"Random_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "                interactions,\n",
    "                preds_raw.round().long(),\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "            # IMIA FCF\n",
    "            # target = fcf.item_grad(user_embedding, item_embedding, interactions)\n",
    "            # preds = interaction_mia_fedrec(\n",
    "            #     lambda I: fcf.item_grad(random_user_emb, item_embedding, I),\n",
    "            #     target,\n",
    "            #     num_data,\n",
    "            #     select_ratio=interactions.mean(),\n",
    "            # )\n",
    "\n",
    "            # metrics.update(\n",
    "            #     \"FCF_IMIA_emb_{embedding_dim}_eps_{epsilon}\",\n",
    "            #     interactions,\n",
    "            #     preds,\n",
    "            # )\n",
    "\n",
    "# metrics.save(\"../output/rec_metrics.csv\")\n",
    "print(metrics.df[[\"name\", \"auc\", \"auc-pr\"]].groupby(\"name\").describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative Filtering + Pruning\n",
    "\n",
    "set_seed()\n",
    "\n",
    "user_ids = data.get_all_user_ids()\n",
    "item_ids = data.get_all_item_ids()\n",
    "user_id_to_idx = {id: idx for idx, id in enumerate(user_ids)}\n",
    "item_id_to_idx = {id: idx for idx, id in enumerate(item_ids)}\n",
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "embedding_dim = 64\n",
    "neg_sample_ratio = 4\n",
    "\n",
    "num_sim_round = 1\n",
    "atk_lr = 1e-01\n",
    "max_iter = 1000\n",
    "num_atk = 5\n",
    "\n",
    "prune_pct = [0.1, 0.3, 0.5, 0.7, 0.9, 0.99]\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "for _ in tqdm(range(num_sim_round)):\n",
    "    user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "    item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "    fcf = CollaborativeFilteringRecommender()\n",
    "    fncf = NeuralCollaborativeFilteringRecommender(embedding_dim, [16, 8])\n",
    "\n",
    "    for user_id in tqdm(user_ids):\n",
    "        # Set up training data\n",
    "        interacted_items = data.get_item_ids_for_users([user_id])[0]\n",
    "        non_interacted_items = data.get_non_interacted_item_ids_for_users([user_id])[0]\n",
    "\n",
    "        num_pos = len(interacted_items)\n",
    "        sampled_non_interacted_items = random.sample(\n",
    "            non_interacted_items,\n",
    "            min(num_pos * neg_sample_ratio, len(non_interacted_items)),\n",
    "        )\n",
    "        num_neg = len(sampled_non_interacted_items)\n",
    "        num_data = num_pos + num_neg\n",
    "\n",
    "        user_embedding = (\n",
    "            user_embeddings(torch.LongTensor([user_id_to_idx[user_id]]))\n",
    "            .detach()\n",
    "            .view(-1)\n",
    "        )\n",
    "        item_embedding = item_embeddings(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    torch.LongTensor([item_id_to_idx[id] for id in interacted_items]),\n",
    "                    torch.LongTensor(\n",
    "                        [item_id_to_idx[id] for id in sampled_non_interacted_items]\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        ).detach()\n",
    "        item_embedding.requires_grad_()\n",
    "        interactions = torch.cat([torch.ones(num_pos), torch.zeros(num_neg)])\n",
    "        random_user_emb = torch.rand(embedding_dim)\n",
    "\n",
    "        for pct in prune_pct:\n",
    "            # FCF Simple\n",
    "            target = fcf.item_grad(user_embedding, item_embedding, interactions)\n",
    "            target = target * (target.abs() >= target.abs().quantile(pct))\n",
    "\n",
    "            preds_raw, _ = reconstruct_interactions(\n",
    "                lambda I: fcf.item_grad(random_user_emb, item_embedding, I),\n",
    "                target,\n",
    "                num_data,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FCF_simple_emb_{embedding_dim}_prune_{pct}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "            # FCF jointly estimate user embedding\n",
    "            preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "                lambda I, U: fcf.item_grad(U, item_embedding, I),\n",
    "                target,\n",
    "                num_data,\n",
    "                private_params_size=embedding_dim,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "            embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FCF_joint_emb_{embedding_dim}_prune_{pct}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "                extra_data={\"est_user_emb_err\": embedding_err},\n",
    "            )\n",
    "\n",
    "            # # FNCF setup\n",
    "            target = fncf.item_grad(user_embedding, item_embedding, interactions)\n",
    "            target = target * (target.abs() >= target.abs().quantile(pct))\n",
    "\n",
    "            mean_norm = torch.linalg.vector_norm(target, dim=1).mean()\n",
    "            norm_scale = max(torch.Tensor([1.0]), torch.Tensor([1e+02]) / mean_norm)\n",
    "            custom_loss = lambda e1, e2: F.pairwise_distance(e1, e2).mean() * norm_scale\n",
    "\n",
    "            # FNCF simple\n",
    "            preds_raw, _ = reconstruct_interactions(\n",
    "                lambda I: fncf.item_grad(random_user_emb, item_embedding, I, create_graph=True),\n",
    "                target,\n",
    "                num_data,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_func=custom_loss,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_simple_emb_{embedding_dim}_prune_{pct}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "            # FNCF jointly estimate user embedding\n",
    "            preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "                lambda I, U: fncf.item_grad(U, item_embedding, I, create_graph=True),\n",
    "                target,\n",
    "                num_data,\n",
    "                private_params_size=embedding_dim,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_func=custom_loss,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "            embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_joint_emb_{embedding_dim}_prune_{pct}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "                extra_data={\"est_user_emb_err\": embedding_err},\n",
    "            )\n",
    "\n",
    "            # FNCF jointly estimate user embedding with neural net params\n",
    "            feature_grad = fncf.feature_grad(user_embedding, item_embedding, interactions)\n",
    "            feature_grad = feature_grad * (feature_grad.abs() > feature_grad.abs().quantile(pct))\n",
    "\n",
    "            preds_raw, user_embedding_est, _ = reconstruct_interactions(\n",
    "                lambda I, U: (\n",
    "                    fncf.item_grad(U, item_embedding, I, create_graph=True),\n",
    "                    fncf.feature_grad(U, item_embedding, I, create_graph=True),\n",
    "                ),\n",
    "                (target, feature_grad),\n",
    "                num_data,\n",
    "                private_params_size=embedding_dim,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_func=lambda t1, t2: custom_loss(t1[0], t2[0]) + F.mse_loss(t1[1], t2[1]),\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "            embedding_err = F.mse_loss(user_embedding_est, user_embedding).item()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_joint_model_emb_{embedding_dim}_prune_{pct}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "                extra_data={\"est_user_emb_err\": embedding_err},\n",
    "            )\n",
    "\n",
    "            # FNCF simple with neural net params\n",
    "            preds_raw, _ = reconstruct_interactions(\n",
    "                lambda I: (\n",
    "                    fncf.item_grad(random_user_emb, item_embedding, I, create_graph=True),\n",
    "                    fncf.feature_grad(random_user_emb, item_embedding, I, create_graph=True),\n",
    "                ),\n",
    "                (target, feature_grad),\n",
    "                num_data,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_func=lambda t1, t2: custom_loss(t1[0], t2[0]) + F.mse_loss(t1[1], t2[1]),\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            metrics.update(\n",
    "                f\"FNCF_simple_model_emb_{embedding_dim}_prune_{pct}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "metrics.save(\"../output/rec_ML100K_pruned_metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
