{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functorch\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torchopt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "from attack import (\n",
    "    reconstruct_interactions,\n",
    ")\n",
    "from dataset import (\n",
    "    MovieLens,\n",
    "    Steam200K,\n",
    ")\n",
    "from ranker import (\n",
    "    NeuralCollaborativeFilteringRecommender,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import (\n",
    "    Metrics,\n",
    "    apply_gaussian_mechanism,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", message='.*make_functional.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset. Choose either ML-100K or STEAM-200K\n",
    "\n",
    "data = MovieLens(\"../dataset/ML-100K/u.data\")\n",
    "# data = Steam200K(\"../dataset/STEAM-200K/steam-200k.csv\")\n",
    "\n",
    "user_ids = data.get_all_user_ids()\n",
    "item_ids = data.get_all_item_ids()\n",
    "user_id_to_idx = {id: idx for idx, id in enumerate(user_ids)}\n",
    "item_id_to_idx = {id: idx for idx, id in enumerate(item_ids)}\n",
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "\n",
    "# Scaled down for artifact eval. Comment this out to run on the entire dataset\n",
    "user_ids = user_ids[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main results in Table V and VI of Section VI.A as well as DP results in Table IX of Section VII.A\n",
    "\n",
    "def set_seed(seed=2023):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)    \n",
    "set_seed()\n",
    "\n",
    "# Initialize embeddings and model\n",
    "embedding_dim = 64\n",
    "neg_sample_ratio = 4\n",
    "user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
    "item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
    "fncf = NeuralCollaborativeFilteringRecommender(embedding_dim, [128, 64, 32])\n",
    "\n",
    "# Reconstruction attack parameters\n",
    "atk_lr = 1e-01\n",
    "max_iter = 1000\n",
    "num_atk = 1\n",
    "\n",
    "# Differential privacy parameters\n",
    "epsilons = [1.0, 20.0, 100.0, 500.0, math.inf]\n",
    "delta = 1e-08\n",
    "sensitivity = 1e-01\n",
    "\n",
    "# Local learning parameters\n",
    "local_epoch = 20\n",
    "local_lr = 0.001\n",
    "reg_factors = [0.0, 1.0] # IMIA defense mu, 0.0 means no defense\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "# Local training algorithm\n",
    "def train_fncf_functional(model, user_embedding, item_embeddings, interactions, num_epoch, lr, reg_factor):\n",
    "    user_embedding.grad = None\n",
    "    item_embeddings.grad = None\n",
    "    func_model, model_params = functorch.make_functional(model)\n",
    "    opt_params = (user_embedding, item_embeddings, *model_params)\n",
    "    # use_accelerated_op=True would be faster but prevent reconstruction for some reasons. Bug?\n",
    "    # eps_root must be set\n",
    "    optimizer = torchopt.FuncOptimizer(torchopt.adam(lr=lr, eps_root=1e-08))\n",
    "    for _ in range(num_epoch):\n",
    "        preds = func_model(opt_params[2:], opt_params[0], opt_params[1])\n",
    "        reg_loss = reg_factor * F.l1_loss(opt_params[1], item_embeddings) # IMIA defense L1 reg term\n",
    "        loss = F.binary_cross_entropy(preds.view(-1), interactions) + reg_loss\n",
    "        opt_params = optimizer.step(loss, opt_params)\n",
    "    return item_embeddings - opt_params[1], opt_params[2:]\n",
    "\n",
    "# Simulate attack on each user\n",
    "for user_id in tqdm(user_ids):\n",
    "    # Sample items and interactions\n",
    "    interacted_items = data.get_item_ids_for_users([user_id])[0]\n",
    "    non_interacted_items = data.get_non_interacted_item_ids_for_users([user_id])[0]\n",
    "\n",
    "    num_pos = len(interacted_items)\n",
    "    sampled_non_interacted_items = random.sample(\n",
    "        non_interacted_items,\n",
    "        min(num_pos * neg_sample_ratio, len(non_interacted_items)),\n",
    "    )\n",
    "    num_neg = len(sampled_non_interacted_items)\n",
    "    num_data = num_pos + num_neg\n",
    "\n",
    "    user_embedding = (\n",
    "        user_embeddings(torch.LongTensor([user_id_to_idx[user_id]]))\n",
    "        .detach()\n",
    "        .view(-1)\n",
    "    )\n",
    "    item_embedding = item_embeddings(\n",
    "        torch.cat(\n",
    "            [\n",
    "                torch.LongTensor([item_id_to_idx[id] for id in interacted_items]),\n",
    "                torch.LongTensor(\n",
    "                    [item_id_to_idx[id] for id in sampled_non_interacted_items]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    ).detach()\n",
    "    user_embedding.requires_grad_()\n",
    "    item_embedding.requires_grad_()\n",
    "    interactions = torch.cat([torch.ones(num_pos), torch.zeros(num_neg)]) # Ground truth interactions\n",
    "    random_user_emb = torch.rand(embedding_dim, requires_grad=True) # The server doesn't know the real user embedding\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "        for reg_factor in reg_factors:\n",
    "            # Local training\n",
    "            target, target_model_params = train_fncf_functional(fncf, user_embedding, item_embedding, interactions, local_epoch, local_lr, reg_factor)\n",
    "            target = apply_gaussian_mechanism(target.detach(), epsilon, delta, sensitivity=sensitivity)\n",
    "\n",
    "            # Attack\n",
    "            mean_norm = torch.linalg.vector_norm(target, dim=1).mean()\n",
    "            norm_scale = max(torch.Tensor([1.0]), torch.Tensor([1e+02]) / mean_norm)\n",
    "            custom_loss = lambda e1, e2: F.pairwise_distance(e1, e2).mean() * norm_scale\n",
    "            preds_raw, _ = reconstruct_interactions(\n",
    "                lambda I: train_fncf_functional(fncf, random_user_emb, item_embedding, I, local_epoch, local_lr, reg_factor)[0] / local_lr,\n",
    "                target / local_lr,\n",
    "                num_data,\n",
    "                lr=atk_lr,\n",
    "                max_iter=max_iter,\n",
    "                num_rounds=num_atk,\n",
    "                loss_fn=custom_loss,\n",
    "                return_raw=True,\n",
    "            )\n",
    "            preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "            # Update attack performance\n",
    "            metrics.update(\n",
    "                f\"FNCF_eps_{epsilon}_IMIA_{reg_factor}\",\n",
    "                interactions,\n",
    "                preds,\n",
    "                preds_raw=preds_raw,\n",
    "            )\n",
    "\n",
    "metrics.save(\"../output/rec_metrics.csv\")\n",
    "metrics.print_summary([\"auc\", \"f1\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
