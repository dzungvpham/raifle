{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox as fb\n",
    "import functorch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import piqa\n",
    "import random\n",
    "import torch\n",
    "import torchopt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "from attack import (\n",
    "    reconstruct_interactions_adam,\n",
    "    optimize_image_manipulation_batches,\n",
    ")\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision.models import (\n",
    "    densenet121, DenseNet121_Weights,\n",
    "    mnasnet1_3, MNASNet1_3_Weights,\n",
    "    regnet_y_800mf, RegNet_Y_800MF_Weights,\n",
    "    resnet18, ResNet18_Weights,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import (\n",
    "    Metrics,\n",
    "    apply_gaussian_mechanism,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", message='.*make_functional.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "normalize = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "unnormalize = transforms.Normalize(\n",
    "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "   std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "\n",
    "def set_seed(seed=2023):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def tensor2img(tensor):\n",
    "    permute_dim = (1, 2, 0) if len(tensor.shape) == 3 else (0, 2, 3, 1)\n",
    "    return unnormalize(tensor).permute(*permute_dim)\n",
    "\n",
    "def display_tensor_as_img(t, unnormalize=True, size=1):\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plt.imshow(tensor2img(t).cpu() if unnormalize else t.cpu(), aspect=\"auto\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "\n",
    "# Model code, weight, and image partitions\n",
    "model_config = {\n",
    "    \"DenseNet121\": (densenet121, DenseNet121_Weights.IMAGENET1K_V1, 2),\n",
    "    \"MNasNet1_3\": (mnasnet1_3, MNASNet1_3_Weights.IMAGENET1K_V1, 2),\n",
    "    \"RegNet_Y_800MF\": (regnet_y_800mf, RegNet_Y_800MF_Weights.IMAGENET1K_V2, 2),\n",
    "    \"ResNet18\": (resnet18, ResNet18_Weights.IMAGENET1K_V1, 2),\n",
    "}\n",
    "chosen_model = \"ResNet18\"\n",
    "opt_basepath = f\"../dataset/CV/{chosen_model}_partitioned\"\n",
    "weights = model_config[chosen_model][1]\n",
    "cv_model = model_config[chosen_model][0](weights=weights).to(device).eval()\n",
    "num_partitions = model_config[chosen_model][2]\n",
    "fb_cv_model = fb.PyTorchModel(cv_model, bounds=(-2.65, 2.65), device=device)\n",
    "for p in cv_model.parameters():\n",
    "    p.grad = None\n",
    "    p.requires_grad_(False)\n",
    "imagenet_categories = {i: v for i, v in enumerate(weights.meta[\"categories\"])}\n",
    "last_layer = list(cv_model.children())[-1]\n",
    "\n",
    "def extract_features(inputs, batch_size=None):\n",
    "    features = []\n",
    "    def getInputs():\n",
    "        def hook(model, input, output):\n",
    "            features.append(input[0])\n",
    "        return hook\n",
    "    h = last_layer.register_forward_hook(getInputs())\n",
    "    if batch_size is None:\n",
    "        cv_model(inputs)\n",
    "    else:\n",
    "        num_batches = math.ceil(inputs.shape[0] / batch_size)\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            cv_model(inputs[start:end,:])\n",
    "    h.remove()\n",
    "    return torch.vstack(features)\n",
    "\n",
    "def dataset_with_indices(cls):\n",
    "    \"\"\"\n",
    "    Modifies the given Dataset class to return a tuple data, target, index\n",
    "    instead of just data, target.\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        data, target = cls.__getitem__(self, index)\n",
    "        return data, target, index\n",
    "\n",
    "    return type(cls.__name__, (cls,), {\n",
    "        '__getitem__': __getitem__,\n",
    "    })\n",
    "\n",
    "ImageNetWithIndices = dataset_with_indices(ImageNet)\n",
    "data = ImageNetWithIndices(\"../dataset/ImageNet/\", split=\"val\", transform=weights.transforms())\n",
    "data_dog = Subset(data, list(range(151*50, 269*50)))\n",
    "image, label, _ = data[0]\n",
    "print(imagenet_categories[label])\n",
    "display_tensor_as_img(image)\n",
    "with torch.no_grad():\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    logits = cv_model(image)\n",
    "    print(f\"Prediction: {imagenet_categories[logits.argmax().cpu().item()]} | Actual: {imagenet_categories[label]}\")\n",
    "    num_features = extract_features(image).shape[1]\n",
    "    print(f\"Number of features extracted: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturb each image and save to disk\n",
    "set_seed(2023)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "def generate_optimized_data(data, base_path, num_partitions):\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "    num_features_per_part = num_features // num_partitions\n",
    "    for partition in range(num_partitions):\n",
    "        Path(f\"{base_path}/p{partition}/\").mkdir(parents=True, exist_ok=True)\n",
    "    for images, _, indices in tqdm(dataloader):\n",
    "        num_items = images.shape[0]\n",
    "        images = images.to(device)\n",
    "        for partition in range(num_partitions):\n",
    "            target = torch.normal(0.0, 4.0, (num_items, num_features), device=device).clip(min=0.0)\n",
    "            target[:,:partition*num_features_per_part].zero_()\n",
    "            target[:,(partition+1)*num_features_per_part:].zero_()\n",
    "\n",
    "            optimized_images = optimize_image_manipulation_batches(\n",
    "                images,  \n",
    "                target,              \n",
    "                extract_features,\n",
    "                batch_size=batch_size,\n",
    "                max_epochs=500,\n",
    "                loss_fn=F.mse_loss,\n",
    "                lr=1e-02,\n",
    "                linf_factor=0.0,\n",
    "                progress_bar=False,\n",
    "            )\n",
    "            \n",
    "            for i in range(num_items):\n",
    "                # When converting from PIL to tensor this results in smaller MSE (around 2.5e-05) \n",
    "                # than using transforms.ToPILImage with unnormalize (1e-04)\n",
    "                save_img = Image.fromarray((255 * tensor2img(optimized_images[i].cpu())).numpy().round().astype(np.uint8))\n",
    "                save_img.save(f\"{base_path}/p{partition}/{indices[i]}.png\")\n",
    "\n",
    "generate_optimized_data(data, opt_basepath, num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_similarity(orig_data, target_path):\n",
    "    psnr = piqa.PSNR()\n",
    "    ssim = piqa.SSIM()\n",
    "    psnrs = []\n",
    "    ssims = []\n",
    "    to_tensor = transforms.ToTensor()\n",
    "\n",
    "    for i in tqdm(range(len(orig_data))):\n",
    "        img = unnormalize(orig_data[i][0]).unsqueeze(0)\n",
    "        target_img = to_tensor(Image.open(f\"{target_path}/p0/{i}.png\")).unsqueeze(0)\n",
    "        psnrs.append(psnr(img, target_img).item())\n",
    "        ssims.append(ssim(img, target_img).item())\n",
    "\n",
    "    return np.mean(psnrs), np.mean(ssims), np.std(psnrs), np.std(ssims)\n",
    "\n",
    "print(measure_similarity(data, opt_basepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2024)\n",
    "model_fns = {\n",
    "    \"linear\": lambda: nn.Linear(num_features, 1, bias=False).to(device),\n",
    "    \"neural2\": lambda: nn.Sequential(\n",
    "        nn.Linear(num_features, 2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2, 1)\n",
    "    ).to(device),\n",
    "    \"neural2_2\": lambda: nn.Sequential(\n",
    "        nn.Linear(num_features, 2, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2, 2, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2, 1, bias=False)\n",
    "    ).to(device),\n",
    "    \"neural2_2_2\": lambda: nn.Sequential(\n",
    "        nn.Linear(num_features, 2, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2, 2, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2, 2, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2, 1, bias=False)\n",
    "    ).to(device),\n",
    "    \"neural4\": lambda: nn.Sequential(\n",
    "        nn.Linear(num_features, 4),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4, 1)\n",
    "    ).to(device),\n",
    "    \"neural8\": lambda: nn.Sequential(\n",
    "        nn.Linear(num_features, 8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8, 1)\n",
    "    ).to(device),\n",
    "}\n",
    "\n",
    "num_sim = 200\n",
    "num_items_per_query = [num_features * 1, num_features * 2, num_features * 4]\n",
    "local_lr = 1e-02\n",
    "local_epoch = 5\n",
    "\n",
    "# Reconstruction\n",
    "num_atk = 1\n",
    "max_iter = 1000\n",
    "atk_lr = 0.1\n",
    "\n",
    "epsilons = [1.0, 20.0, 100.0, 500.0, math.inf]\n",
    "delta = 1e-08\n",
    "sensitivity = 0.05\n",
    "\n",
    "adv_attacks = {\n",
    "    \"FGSM\": fb.attacks.FGSM(),\n",
    "}\n",
    "adv_epsilons = [0.1]\n",
    "\n",
    "metrics = Metrics()\n",
    "extract_batch_size = 512\n",
    "\n",
    "def train(model, features, interactions):\n",
    "    func_model, model_params = functorch.make_functional(model)\n",
    "    opt_params = model_params\n",
    "    optimizer = torchopt.FuncOptimizer(torchopt.sgd(lr=local_lr))\n",
    "    for _ in range(local_epoch):\n",
    "        preds = func_model(opt_params, features)\n",
    "        loss = F.binary_cross_entropy_with_logits(preds.view(-1), interactions)\n",
    "        opt_params = optimizer.step(loss, opt_params)\n",
    "    model_params = torch.cat([p.view(-1) for p in model_params])\n",
    "    opt_params = torch.cat([p.view(-1) for p in opt_params])\n",
    "    return model_params - opt_params\n",
    "\n",
    "def adv_perturb(fmodel, images, labels, attack, batch_size=None, **kwargs):\n",
    "    if batch_size is None:\n",
    "        _, clipped_advs, success = attack(fmodel, images, labels, **kwargs)\n",
    "    else:\n",
    "        clipped_advs_list = []\n",
    "        success_list = []\n",
    "        num_images = images.shape[0]\n",
    "        num_batches = int(math.ceil(num_images / batch_size))\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_images = images[start_idx:end_idx]\n",
    "            batch_labels = labels[start_idx:end_idx]\n",
    "            _, batch_clipped_advs, batch_success = attack(fmodel, batch_images, batch_labels, **kwargs)\n",
    "            clipped_advs_list.append([batch for batch in batch_clipped_advs])\n",
    "            success_list.append(batch_success)\n",
    "            torch.cuda.empty_cache()\n",
    "        clipped_advs = [\n",
    "            torch.vstack([clipped_advs_list[j][i] for j in range(num_batches)]) for i in range(len(clipped_advs_list[0]))\n",
    "        ]\n",
    "        success = torch.hstack(success_list)\n",
    "\n",
    "    return clipped_advs, success\n",
    "\n",
    "def load_opt_images(indices, num_partitions):\n",
    "    imgs = []\n",
    "    n = len(indices) // num_partitions\n",
    "    for c, i in enumerate(indices):\n",
    "        partition = c // n\n",
    "        imgs.append(normalize(Image.open(f\"{opt_basepath}/p{partition}/{i}.png\")))\n",
    "    return torch.stack(imgs)\n",
    "\n",
    "def simulate_attack(data, epsilons, num_items):\n",
    "    dataloader = DataLoader(data, batch_size=num_items, shuffle=True)\n",
    "    images, labels, indices = next(iter(dataloader))\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    num_items = images.shape[0]\n",
    "    target_interactions = torch.randint(0, 2, (num_items,)).float().to(device)\n",
    "    grouped_train_data_dict = {\n",
    "        \"no_adm\": extract_features(images, extract_batch_size),\n",
    "    }\n",
    "    \n",
    "    for attack_name, attack in adv_attacks.items():\n",
    "        adv_images_list, _, = adv_perturb(fb_cv_model, images, labels, attack, batch_size=16, epsilons=adv_epsilons)\n",
    "        for eps, adv_images in zip(adv_epsilons, adv_images_list):\n",
    "            grouped_train_data_dict[f\"adm_{attack_name}_{eps}\"] = extract_features(adv_images, extract_batch_size)\n",
    "\n",
    "    optimized_images = load_opt_images(indices, num_partitions).to(device)\n",
    "    grouped_train_data_dict[\"adm_opt\"] = extract_features(optimized_images, extract_batch_size)\n",
    "    del optimized_images    \n",
    "\n",
    "    for model_name, model_fn in model_fns.items():\n",
    "        model = model_fn()\n",
    "        raw_target_dict = {\n",
    "            key: train(model, train_features, target_interactions).detach() for\n",
    "                key, train_features in grouped_train_data_dict.items()\n",
    "        }\n",
    "\n",
    "        for epsilon in epsilons:\n",
    "            for key, raw_target in raw_target_dict.items():\n",
    "                target = (apply_gaussian_mechanism(raw_target.detach().cpu(), epsilon, delta, sensitivity)).to(device)\n",
    "                train_features = grouped_train_data_dict[key]\n",
    "                preds_raw = reconstruct_interactions_adam(\n",
    "                    lambda I: train(model, train_features, I) / local_lr,\n",
    "                    target / local_lr,\n",
    "                    num_items,\n",
    "                    num_epochs=300,\n",
    "                    device=device,\n",
    "                    lr=atk_lr,\n",
    "                )\n",
    "                preds_raw = preds_raw.detach().cpu()\n",
    "                preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "                metrics.update(\n",
    "                    f\"{model_name}_{num_items}_items_eps_{epsilon}_{key}\",\n",
    "                    target_interactions.cpu(),\n",
    "                    preds,\n",
    "                    preds_raw=preds_raw,\n",
    "                )\n",
    "\n",
    "for _ in tqdm(range(num_sim)):\n",
    "    for num_items in num_items_per_query:\n",
    "        simulate_attack(data, epsilons, num_items)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "metrics.print_summary()\n",
    "# metrics.save(\"../output/ltr_cv_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "set_seed(2024)\n",
    "model_fns = {\n",
    "    \"neural8\": lambda: nn.Sequential(\n",
    "        nn.Linear(num_features, 8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8, 1)\n",
    "    ).to(device),\n",
    "}\n",
    "\n",
    "num_sim = 1000\n",
    "num_items_per_query = [num_features * 4]\n",
    "local_lr = 1e-02\n",
    "local_epoch = 5\n",
    "\n",
    "adv_attacks = {\n",
    "    \"FGSM\": fb.attacks.FGSM(),\n",
    "}\n",
    "adv_epsilons = [0.1]\n",
    "\n",
    "metrics = Metrics()\n",
    "extract_batch_size = 512\n",
    "\n",
    "grads_dict = {\n",
    "    \"no_adm\": [],\n",
    "    \"adm_opt\": [],\n",
    "}\n",
    "\n",
    "def train(model, features, interactions):\n",
    "    func_model, model_params = functorch.make_functional(model)\n",
    "    opt_params = model_params\n",
    "    optimizer = torchopt.FuncOptimizer(torchopt.sgd(lr=local_lr))\n",
    "    for _ in range(local_epoch):\n",
    "        preds = func_model(opt_params, features)\n",
    "        loss = F.binary_cross_entropy_with_logits(preds.view(-1), interactions)\n",
    "        opt_params = optimizer.step(loss, opt_params)\n",
    "    model_params = torch.cat([p.view(-1) for p in model_params])\n",
    "    opt_params = torch.cat([p.view(-1) for p in opt_params])\n",
    "    return model_params - opt_params\n",
    "\n",
    "def adv_perturb(fmodel, images, labels, attack, batch_size=None, **kwargs):\n",
    "    if batch_size is None:\n",
    "        _, clipped_advs, success = attack(fmodel, images, labels, **kwargs)\n",
    "    else:\n",
    "        clipped_advs_list = []\n",
    "        success_list = []\n",
    "        num_images = images.shape[0]\n",
    "        num_batches = int(math.ceil(num_images / batch_size))\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_images = images[start_idx:end_idx]\n",
    "            batch_labels = labels[start_idx:end_idx]\n",
    "            _, batch_clipped_advs, batch_success = attack(fmodel, batch_images, batch_labels, **kwargs)\n",
    "            clipped_advs_list.append([batch for batch in batch_clipped_advs])\n",
    "            success_list.append(batch_success)\n",
    "            torch.cuda.empty_cache()\n",
    "        clipped_advs = [\n",
    "            torch.vstack([clipped_advs_list[j][i] for j in range(num_batches)]) for i in range(len(clipped_advs_list[0]))\n",
    "        ]\n",
    "        success = torch.hstack(success_list)\n",
    "\n",
    "    return clipped_advs, success\n",
    "\n",
    "def load_opt_images(indices, num_partitions):\n",
    "    imgs = []\n",
    "    n = len(indices) // num_partitions\n",
    "    for c, i in enumerate(indices):\n",
    "        partition = c // n\n",
    "        imgs.append(normalize(Image.open(f\"{opt_basepath}/p{partition}/{i}.png\")))\n",
    "    return torch.stack(imgs)\n",
    "\n",
    "def simulate_attack(data, num_items):\n",
    "    dataloader = DataLoader(data, batch_size=num_items, shuffle=True)\n",
    "    images, labels, indices = next(iter(dataloader))\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    num_items = images.shape[0]\n",
    "    target_interactions = torch.randint(0, 2, (num_items,)).float().to(device)\n",
    "    grouped_train_data_dict = {\n",
    "        \"no_adm\": extract_features(images, extract_batch_size),\n",
    "    }\n",
    "\n",
    "    optimized_images = load_opt_images(indices, num_partitions).to(device)\n",
    "    grouped_train_data_dict[\"adm_opt\"] = extract_features(optimized_images, extract_batch_size)\n",
    "    del optimized_images    \n",
    "\n",
    "    for model_name, model_fn in model_fns.items():\n",
    "        model = model_fn()\n",
    "        for key, train_features in grouped_train_data_dict.items():\n",
    "            grads_dict[key].append(train(model, train_features, target_interactions).detach().cpu())\n",
    "\n",
    "for _ in tqdm(range(num_sim)):\n",
    "    for num_items in num_items_per_query:\n",
    "        simulate_attack(data, num_items)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "num_grads = len(grads_dict[\"no_adm\"])\n",
    "grads = torch.vstack([\n",
    "    torch.stack(grads_dict[\"no_adm\"]),\n",
    "    torch.stack(grads_dict[\"adm_opt\"]),\n",
    "]).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "handles = []  # Collect legend handles\n",
    "labels = []   # Collect legend labels\n",
    "\n",
    "for i, perplexity in enumerate([50, 100, 200, 400]):\n",
    "    visualizer = TSNE(n_components=2, perplexity=perplexity)\n",
    "    results = visualizer.fit_transform(grads)\n",
    "\n",
    "    ax = axes[i]\n",
    "    orange_scatter = ax.scatter(results[:num_grads, 0], results[:num_grads, 1], c=\"orange\", label=\"No ADM\", alpha=1.0, marker='.')\n",
    "    blue_scatter = ax.scatter(results[num_grads:, 0], results[num_grads:, 1], c=\"blue\", label=\"ADM\", alpha=0.5, marker='.')\n",
    "    ax.set_title(f'Perplexity = {perplexity}')\n",
    "    ax.set_xlabel('Component 1')\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Component 2')\n",
    "        handles.append(orange_scatter)\n",
    "        handles.append(blue_scatter)\n",
    "        labels.append(\"No ADM\")\n",
    "        labels.append(\"ADM\")\n",
    "\n",
    "lgd = fig.legend(handles, labels, loc=\"lower center\", bbox_to_anchor=(0.5, -0.1), ncols=2)\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"../plots/tsne_resnet_neural8_x4.pdf\", bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
