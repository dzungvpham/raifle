{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox as fb\n",
    "import functorch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import piqa\n",
    "import random\n",
    "import torch\n",
    "import torchopt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "from attack import (\n",
    "    reconstruct_interactions,\n",
    "    optimize_image_manipulation_batches,\n",
    ")\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision.models import (\n",
    "    regnet_y_400mf, RegNet_Y_400MF_Weights,\n",
    "    resnet18, ResNet18_Weights,\n",
    "    mobilenet_v3_small, MobileNet_V3_Small_Weights,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import (\n",
    "    Metrics,\n",
    "    apply_gaussian_mechanism,\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", message='.*make_functional.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=2023):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "normalize = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "unnormalize = transforms.Normalize(\n",
    "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "   std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "\n",
    "def tensor2img(tensor):\n",
    "    permute_dim = (1, 2, 0) if len(tensor.shape) == 3 else (0, 2, 3, 1)\n",
    "    return unnormalize(tensor).permute(*permute_dim)\n",
    "\n",
    "def display_tensor_as_img(t, unnormalize=True, size=1):\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plt.imshow(tensor2img(t).cpu() if unnormalize else t.cpu(), aspect=\"auto\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "\n",
    "model_config = {\n",
    "    \"RegNet_Y_400MF\": (regnet_y_400mf, RegNet_Y_400MF_Weights.IMAGENET1K_V2),\n",
    "    \"ResNet18\": (resnet18, ResNet18_Weights.IMAGENET1K_V1),\n",
    "    \"MobileNet_V3_Small\": (mobilenet_v3_small, MobileNet_V3_Small_Weights.IMAGENET1K_V1),\n",
    "}\n",
    "chosen_model = \"RegNet_Y_400MF\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = model_config[chosen_model][1]\n",
    "cv_model = model_config[chosen_model][0](weights=weights).to(device).eval()\n",
    "opt_basepath = f\"../dataset/CV/RegNet_Y_400MF\"\n",
    "fb_cv_model = fb.PyTorchModel(cv_model, bounds=(-2.65, 2.65), device=device)\n",
    "feature_extractor = nn.Sequential(*(list(cv_model.children())[:-1])).to(device).eval()\n",
    "for p in cv_model.parameters():\n",
    "    p.grad = None\n",
    "    p.requires_grad_(False)\n",
    "for p in feature_extractor.parameters():\n",
    "    p.grad = None\n",
    "    p.requires_grad_(False) \n",
    "imagenet_categories = {i: v for i, v in enumerate(weights.meta[\"categories\"])}\n",
    "\n",
    "def dataset_with_indices(cls):\n",
    "    \"\"\"\n",
    "    Modifies the given Dataset class to return a tuple data, target, index\n",
    "    instead of just data, target.\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        data, target = cls.__getitem__(self, index)\n",
    "        return data, target, index\n",
    "\n",
    "    return type(cls.__name__, (cls,), {\n",
    "        '__getitem__': __getitem__,\n",
    "    })\n",
    "\n",
    "ImageNetWithIndices = dataset_with_indices(ImageNet)\n",
    "data = ImageNetWithIndices(\"../dataset/ImageNet/\", split=\"val\", transform=weights.transforms())\n",
    "data_dog = Subset(data, list(range(151*50, 269*50)))\n",
    "image, label, _ = data[0]\n",
    "print(imagenet_categories[label])\n",
    "display_tensor_as_img(image)\n",
    "with torch.no_grad():\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    logits = cv_model(image)\n",
    "    print(f\"Prediction: {imagenet_categories[logits.argmax().cpu().item()]} | Actual: {imagenet_categories[label]}\")\n",
    "    num_features = feature_extractor(image).shape[1]\n",
    "    print(f\"Number of features extracted: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturb each image and save to disk\n",
    "set_seed(2023)\n",
    "\n",
    "batch_size = 256\n",
    "def extract_features(images):\n",
    "    return feature_extractor(images).squeeze()[:, :num_features]\n",
    "\n",
    "def generate_optimized_data(data, base_path):\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "    idx = 0\n",
    "    for images, _, _ in tqdm(dataloader):\n",
    "        num_items = images.shape[0]\n",
    "        images = images.to(device)\n",
    "        \n",
    "        optimized_images = optimize_image_manipulation_batches(\n",
    "            images,\n",
    "            torch.normal(0.0, 4.0, (num_items, num_features), device=device).clip(min=0.0),\n",
    "            extract_features,\n",
    "            batch_size=batch_size,\n",
    "            max_epochs=500,\n",
    "            loss_fn=F.mse_loss,\n",
    "            lr=1e-02,\n",
    "            linf_factor=0.0,\n",
    "            progress_bar=False,\n",
    "        )\n",
    "\n",
    "        for i in range(num_items):\n",
    "            # When converting from PIL to tensor this results in smaller MSE (around 2.5e-05) \n",
    "            # than using transforms.ToPILImage with unnormalize (1e-04)\n",
    "            save_img = Image.fromarray((255 * tensor2img(optimized_images[i].cpu())).numpy().round().astype(np.uint8))\n",
    "            save_img.save(f\"{base_path}/{idx}.jpg\")\n",
    "            idx += 1\n",
    "\n",
    "generate_optimized_data(data, opt_basepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_similarity(orig_data, target_path):\n",
    "    psnr = piqa.PSNR()\n",
    "    ssim = piqa.SSIM()\n",
    "    psnrs = []\n",
    "    ssims = []\n",
    "    to_tensor = transforms.ToTensor()\n",
    "\n",
    "    for i in tqdm(range(len(orig_data))):\n",
    "        img = unnormalize(orig_data[i][0]).unsqueeze(0)\n",
    "        target_img = to_tensor(Image.open(f\"{target_path}/{i}.jpg\")).unsqueeze(0)\n",
    "        psnrs.append(psnr(img, target_img).item())\n",
    "        ssims.append(ssim(img, target_img).item())\n",
    "        break\n",
    "\n",
    "    return np.mean(psnrs), np.mean(ssims)\n",
    "\n",
    "print(measure_similarity(data, opt_basepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2024)\n",
    "\n",
    "model_fns = {\n",
    "    \"linear\": lambda: nn.Linear(num_features, 1, bias=False).to(device),\n",
    "    \"neural2\": lambda: nn.Sequential(\n",
    "        nn.Linear(num_features, 2, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2, 1, bias=False)\n",
    "    ).to(device),\n",
    "    \"neural3\": lambda: nn.Sequential(\n",
    "        nn.Linear(num_features, 3, bias=False),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(3, 1, bias=False)\n",
    "    ).to(device),\n",
    "}\n",
    "\n",
    "num_sim = 10\n",
    "num_items_per_query = [num_features * 1, num_features * 2, num_features * 3]\n",
    "local_lr = 1e-02\n",
    "local_epoch = 5\n",
    "\n",
    "# Reconstruction\n",
    "num_atk = 1\n",
    "max_iter = 1000\n",
    "atk_lr = 0.1\n",
    "\n",
    "epsilons = [1.0, 20.0, 100.0, 500.0, math.inf]\n",
    "delta = 1e-08\n",
    "sensitivity = 0.05\n",
    "\n",
    "adv_attacks = {\n",
    "    \"FGSM\": fb.attacks.FGSM(),\n",
    "}\n",
    "adv_epsilons = [0.1]\n",
    "\n",
    "metrics = Metrics()\n",
    "\n",
    "def extract_features(images):\n",
    "    return feature_extractor(images).squeeze()[:, :num_features]\n",
    "\n",
    "def train(model, features, interactions):\n",
    "    func_model, model_params = functorch.make_functional(model)\n",
    "    opt_params = model_params\n",
    "    optimizer = torchopt.FuncOptimizer(torchopt.sgd(lr=local_lr))\n",
    "    for _ in range(local_epoch):\n",
    "        preds = func_model(opt_params, features)\n",
    "        loss = F.binary_cross_entropy_with_logits(preds.view(-1), interactions)\n",
    "        opt_params = optimizer.step(loss, opt_params)\n",
    "    model_params = torch.cat([p.view(-1) for p in model_params])\n",
    "    opt_params = torch.cat([p.view(-1) for p in opt_params])\n",
    "    return model_params - opt_params\n",
    "\n",
    "def adv_perturb(fmodel, images, labels, attack, batch_size=None, **kwargs):\n",
    "    if batch_size is None:\n",
    "        _, clipped_advs, success = attack(fmodel, images, labels, **kwargs)\n",
    "    else:\n",
    "        clipped_advs_list = []\n",
    "        success_list = []\n",
    "        num_images = images.shape[0]\n",
    "        num_batches = int(math.ceil(num_images / batch_size))\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_images = images[start_idx:end_idx]\n",
    "            batch_labels = labels[start_idx:end_idx]\n",
    "            _, batch_clipped_advs, batch_success = attack(fmodel, batch_images, batch_labels, **kwargs)\n",
    "            clipped_advs_list.append([batch for batch in batch_clipped_advs])\n",
    "            success_list.append(batch_success)\n",
    "            torch.cuda.empty_cache()\n",
    "        clipped_advs = [\n",
    "            torch.vstack([clipped_advs_list[j][i] for j in range(num_batches)]) for i in range(len(clipped_advs_list[0]))\n",
    "        ]\n",
    "        success = torch.hstack(success_list)\n",
    "\n",
    "    clean_acc = fb.accuracy(fmodel, images, labels)\n",
    "    return clipped_advs, success, clean_acc\n",
    "\n",
    "def load_opt_images(indices):\n",
    "    imgs = []\n",
    "    for i in indices:\n",
    "        imgs.append(normalize(Image.open(f\"{opt_basepath}/{i}.jpg\")))\n",
    "    return torch.stack(imgs)\n",
    "\n",
    "def simulate_attack(data, epsilons, num_items):\n",
    "    dataloader = DataLoader(data, batch_size=num_items, shuffle=True)\n",
    "    images, labels, indices = next(iter(dataloader))\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    num_items = images.shape[0]\n",
    "    target_interactions = torch.randint(0, 2, (num_items,)).float().to(device)\n",
    "    extracted_features = extract_features(images)\n",
    "    grouped_train_data_dict = {\n",
    "        \"no_adm\": extracted_features,\n",
    "    }\n",
    "    \n",
    "    for attack_name, attack in adv_attacks.items():\n",
    "        adv_images_list, _, _ = adv_perturb(fb_cv_model, images, labels, attack, batch_size=16, epsilons=adv_epsilons)\n",
    "        for eps, adv_images in zip(adv_epsilons, adv_images_list):\n",
    "            grouped_train_data_dict[f\"adm_{attack_name}_{eps}\"] = extract_features(adv_images)\n",
    "\n",
    "    optimized_images = load_opt_images(indices).to(device)\n",
    "    grouped_train_data_dict[\"adm_opt\"] = extract_features(optimized_images)\n",
    "    del optimized_images\n",
    "\n",
    "    for model_name, model_fn in model_fns.items():\n",
    "        model = model_fn()\n",
    "        raw_target_dict = {\n",
    "            key: train(model, train_features, target_interactions).detach() for\n",
    "                key, train_features in grouped_train_data_dict.items()\n",
    "        }\n",
    "\n",
    "        for epsilon in epsilons:\n",
    "            for key, raw_target in raw_target_dict.items():\n",
    "                target = (apply_gaussian_mechanism(raw_target.detach().cpu(), epsilon, delta, sensitivity)).to(device)\n",
    "                train_features = grouped_train_data_dict[key]\n",
    "                preds_raw, _ = reconstruct_interactions(\n",
    "                    lambda I: train(model, train_features, I) / local_lr,\n",
    "                    target / local_lr,\n",
    "                    num_items,\n",
    "                    lr=atk_lr,\n",
    "                    max_iter=max_iter,\n",
    "                    num_rounds=num_atk,\n",
    "                    return_raw=True,\n",
    "                    device=device,\n",
    "                )\n",
    "                preds_raw = preds_raw.detach().cpu()\n",
    "                preds = preds_raw.sigmoid().round().long()\n",
    "\n",
    "                metrics.update(\n",
    "                    f\"{model_name}_{num_items}_items_eps_{epsilon}_{key}\",\n",
    "                    target_interactions.cpu(),\n",
    "                    preds,\n",
    "                    preds_raw=preds_raw,\n",
    "                )\n",
    "\n",
    "for _ in tqdm(range(num_sim)):\n",
    "    for num_items in num_items_per_query:\n",
    "        simulate_attack(data, epsilons, num_items)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "metrics.print_summary()\n",
    "# metrics.save(\"../output/ltr_cv_metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
