{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hgkJcWHg-4C_"
      },
      "outputs": [],
      "source": [
        "!pip install diffprivlib\n",
        "!pip install torchopt\n",
        "\n",
        "import functorch\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torchopt\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import warnings\n",
        "import json\n",
        "import os.path\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import traceback\n",
        "from torch.func import grad, vmap\n",
        "from tqdm import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from diffprivlib.mechanisms import (\n",
        "    Gaussian,\n",
        "    GaussianAnalytic,\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    average_precision_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        ")\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message='.*make_functional.*')\n",
        "\n",
        "class RecommendationDataset:\n",
        "    def __init__(self, df, user_id_col, item_id_col, rating_col):\n",
        "        self.user_ids = np.sort(df[user_id_col].unique()).tolist()\n",
        "        self.item_ids = np.sort(df[item_id_col].unique()).tolist()\n",
        "        self.user_to_item_rating_map = df.groupby(user_id_col).apply(\n",
        "            lambda x: dict(zip(x[item_id_col], x[rating_col]))\n",
        "        )\n",
        "\n",
        "    def get_all_user_ids(self) -> List:\n",
        "        return self.user_ids\n",
        "\n",
        "    def get_all_item_ids(self) -> List:\n",
        "        return self.item_ids\n",
        "\n",
        "    def get_item_ids_for_users(self, user_ids: List) -> List[List]:\n",
        "        return (\n",
        "            self.user_to_item_rating_map.loc[user_ids]\n",
        "            .apply(lambda x: list(x.keys()))\n",
        "            .tolist()\n",
        "        )\n",
        "\n",
        "    def get_non_interacted_item_ids_for_users(self, user_ids: List) -> List[List]:\n",
        "        return (\n",
        "            self.user_to_item_rating_map.loc[user_ids]\n",
        "            .apply(lambda x: list(set(self.get_all_item_ids()) - set(x.keys())))\n",
        "            .tolist()\n",
        "        )\n",
        "\n",
        "    # Return a list of dictionary from item id to rating\n",
        "    def get_item_ratings_for_users(self, user_ids: List) -> List[Dict]:\n",
        "        return self.user_to_item_rating_map.loc[user_ids].tolist()\n",
        "\n",
        "\n",
        "class MovieLens(RecommendationDataset):\n",
        "    def __init__(self, path=\"../dataset/ML-100K/u.data\", sep=\"\\t\"):\n",
        "        df = pd.read_csv(\n",
        "            path,\n",
        "            sep=sep,\n",
        "            header=None,\n",
        "            names=[\"user_id\", \"item_id\", \"rating\"],\n",
        "            usecols=[\"user_id\", \"item_id\", \"rating\"],\n",
        "        )\n",
        "        super().__init__(df, \"user_id\", \"item_id\", \"rating\")\n",
        "\n",
        "\n",
        "class Steam200K(RecommendationDataset):\n",
        "    def __init__(self, path=\"../dataset/STEAM-200K/steam-200k.csv\"):\n",
        "        df = pd.read_csv(\n",
        "            path,\n",
        "            header=None,\n",
        "            names=[\"user_id\", \"item_id\", \"behavior\"],\n",
        "            usecols=[\"user_id\", \"item_id\", \"behavior\"],\n",
        "        )\n",
        "        df = df.drop_duplicates(subset=[\"user_id\", \"item_id\"], ignore_index=True)\n",
        "        df[\"item_id\"], _ = pd.factorize(df[\"item_id\"])\n",
        "        df[\"rating\"] = 1\n",
        "        super().__init__(df, \"user_id\", \"item_id\", \"rating\")\n",
        "\n",
        "def reconstruct_interactions(\n",
        "    trainer,\n",
        "    target_params,\n",
        "    num_items,\n",
        "    private_params_size=0,\n",
        "    loss_fn=F.mse_loss,\n",
        "    num_rounds=1,\n",
        "    return_raw=False,\n",
        "    prior_penalty=None,\n",
        "    device=torch.device(\"cpu\"),\n",
        "    **kwargs,\n",
        "):\n",
        "    if prior_penalty is None:\n",
        "        prior_penalty = lambda _: 0.0\n",
        "\n",
        "    best_loss = math.inf\n",
        "    best_opt_params = None\n",
        "\n",
        "    for _ in range(num_rounds):\n",
        "        opt_params = nn.Parameter(torch.rand(num_items + private_params_size, device=device) * 2 - 1)\n",
        "        optimizer = optim.LBFGS([opt_params], line_search_fn=\"strong_wolfe\", **kwargs)\n",
        "\n",
        "        def calc_loss():\n",
        "            optimizer.zero_grad()\n",
        "            interactions = opt_params[:num_items].sigmoid()\n",
        "            shadow_params = (\n",
        "                trainer(interactions)\n",
        "                if private_params_size == 0\n",
        "                else trainer(interactions, opt_params[num_items:])\n",
        "            )\n",
        "            loss = loss_fn(shadow_params, target_params) + prior_penalty(interactions)\n",
        "            loss.backward(inputs=[opt_params])\n",
        "            return loss\n",
        "\n",
        "        try:\n",
        "            optimizer.step(calc_loss)\n",
        "        except Exception:\n",
        "            print(\"An exception occurred in the optimization step!\")\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "        optimizer_state = optimizer.state[list(optimizer.state)[0]]\n",
        "        if \"prev_loss\" not in optimizer_state:\n",
        "            print(\"Optimization did not take any step!\")\n",
        "            continue\n",
        "        else:\n",
        "            cur_loss = optimizer_state[\"prev_loss\"]\n",
        "\n",
        "        if cur_loss < best_loss:\n",
        "            best_loss = cur_loss\n",
        "            best_opt_params = opt_params.detach()\n",
        "\n",
        "    if best_opt_params is None:\n",
        "        print(\"Optimization failed! Defaulting to random guessing.\")\n",
        "        best_opt_params = torch.rand(num_items + private_params_size) * 2 - 1\n",
        "\n",
        "    if private_params_size == 0:\n",
        "        if return_raw:\n",
        "            return best_opt_params, best_loss\n",
        "        else:\n",
        "            return best_opt_params.sigmoid().round().long(), best_loss\n",
        "    else:\n",
        "        if return_raw:\n",
        "            return best_opt_params[:num_items], best_opt_params[num_items:], best_loss\n",
        "        else:\n",
        "            return (\n",
        "                best_opt_params[:num_items].sigmoid().round().long(),\n",
        "                best_opt_params[num_items:],\n",
        "                best_loss,\n",
        "            )\n",
        "\n",
        "class NeuralCollaborativeFilteringRecommender(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_sizes, bias=True):\n",
        "        super().__init__()\n",
        "        self.first_layer = nn.Linear(embedding_size * 2, hidden_sizes[0], bias=bias)\n",
        "        self.fc_layers = []\n",
        "        for i in range(len(hidden_sizes) - 1):\n",
        "            self.fc_layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1], bias=bias))\n",
        "        self.fc_layers = nn.ModuleList(self.fc_layers)\n",
        "        self.final_layer = nn.Linear(hidden_sizes[-1], 1, bias=bias)\n",
        "\n",
        "    def forward(self, user_embedding, item_embeddings):\n",
        "        embeddings = torch.cat(\n",
        "            [user_embedding.expand(item_embeddings.shape[0], -1), item_embeddings], dim=1\n",
        "        )\n",
        "        res = F.relu(self.first_layer(embeddings))\n",
        "        for layer in self.fc_layers:\n",
        "            res = F.relu(layer(res))\n",
        "        return F.sigmoid(self.final_layer(res))\n",
        "\n",
        "    def item_grad(self, user_embedding, item_embeddings, interactions, create_graph=False):\n",
        "        self.eval()\n",
        "        item_embeddings.grad = None\n",
        "        preds = self.forward(user_embedding, item_embeddings)\n",
        "        loss = F.binary_cross_entropy(preds.view(-1), interactions)\n",
        "        return autograd.grad(loss, item_embeddings, create_graph=create_graph)[0]\n",
        "\n",
        "    def feature_grad(self, user_embedding, item_embeddings, interactions, create_graph=False):\n",
        "        self.train()\n",
        "        for p in self.parameters():\n",
        "            p.grad = None\n",
        "        preds = self.forward(user_embedding, item_embeddings)\n",
        "        loss = F.binary_cross_entropy(preds.view(-1), interactions)\n",
        "        grads = autograd.grad(loss, list(self.parameters()), create_graph=create_graph)\n",
        "        return torch.cat([t.flatten() for t in grads])\n",
        "\n",
        "class Metrics:\n",
        "    def __init__(self, path=None):\n",
        "        if path is not None:\n",
        "            self.df = pd.read_csv(path)\n",
        "        else:\n",
        "            self.df = pd.DataFrame(\n",
        "                {\n",
        "                    \"name\": [],\n",
        "                    \"accuracy\": [],\n",
        "                    \"f1\": [],\n",
        "                    \"precision\": [],\n",
        "                    \"recall\": [],\n",
        "                    \"auc\": [],\n",
        "                    \"auc-pr\": [],\n",
        "                    \"extra_data\": [],\n",
        "                }\n",
        "            )\n",
        "\n",
        "    def update(self, name, target, preds, preds_raw=None, extra_data={}):\n",
        "        row = {\n",
        "            \"name\": name,\n",
        "            \"accuracy\": accuracy_score(target, preds),\n",
        "            \"f1\": f1_score(target, preds),\n",
        "            \"precision\": precision_score(target, preds, zero_division=0),\n",
        "            \"recall\": recall_score(target, preds),\n",
        "            \"auc\": None if preds_raw is None else roc_auc_score(target, preds_raw),\n",
        "            \"auc-pr\": None\n",
        "            if preds_raw is None\n",
        "            else average_precision_score(target, preds_raw),\n",
        "            \"extra_data\": json.dumps(extra_data),\n",
        "        }\n",
        "        self.df.loc[len(self.df.index), :] = row\n",
        "\n",
        "    def get_dataframe(self):\n",
        "        return self.df\n",
        "\n",
        "    def save(self, path):\n",
        "        self.df.to_csv(path, index=False)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.df = pd.read_csv(path)\n",
        "\n",
        "    def print_summary(self, metrics=[\"auc\"]):\n",
        "        print(self.df[[\"name\"] + metrics].groupby(\"name\").describe().to_string())\n",
        "\n",
        "def apply_gaussian_mechanism(input, epsilon, delta, sensitivity, scale_only=False):\n",
        "    if math.isinf(epsilon):\n",
        "        return input\n",
        "    # Clip L2 norm to 0.5 * sensitivity (since global L2 sensitivity = 2 * max L2 norm)\n",
        "    output = input * torch.minimum(torch.tensor(1.0), 0.5 * sensitivity / torch.linalg.vector_norm(input))\n",
        "    if scale_only:\n",
        "        return output\n",
        "\n",
        "    # Add noise\n",
        "    mechanism = (Gaussian if epsilon <= 1.0 else GaussianAnalytic)(\n",
        "        epsilon=epsilon, delta=delta, sensitivity=sensitivity\n",
        "    )\n",
        "    return output.apply_(mechanism.randomise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qykmoNgyApEa"
      },
      "outputs": [],
      "source": [
        "# Load dataset. Choose either ML-100K or STEAM-200K\n",
        "\n",
        "data = MovieLens(\"./u.data\")\n",
        "# data = Steam200K(\"../dataset/STEAM-200K/steam-200k.csv\")\n",
        "\n",
        "user_ids = data.get_all_user_ids()\n",
        "item_ids = data.get_all_item_ids()\n",
        "user_id_to_idx = {id: idx for idx, id in enumerate(user_ids)}\n",
        "item_id_to_idx = {id: idx for idx, id in enumerate(item_ids)}\n",
        "num_users = len(user_ids)\n",
        "num_items = len(item_ids)\n",
        "\n",
        "# Scaled down for artifact eval. Comment this out to run on the entire dataset\n",
        "user_ids = user_ids[:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnrzwDknA-IA"
      },
      "outputs": [],
      "source": [
        "# Main results in Table V and VI of Section VI.A as well as DP results in Table IX of Section VII.A\n",
        "\n",
        "def set_seed(seed=2023):\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "set_seed()\n",
        "\n",
        "# Initialize embeddings and model\n",
        "embedding_dim = 64\n",
        "neg_sample_ratio = 4\n",
        "user_embeddings = nn.Embedding(num_users, embedding_dim)\n",
        "item_embeddings = nn.Embedding(num_items, embedding_dim)\n",
        "fncf = NeuralCollaborativeFilteringRecommender(embedding_dim, [128, 64, 32])\n",
        "\n",
        "# Reconstruction attack parameters\n",
        "atk_lr = 1e-01\n",
        "max_iter = 1000\n",
        "num_atk = 1\n",
        "\n",
        "# Differential privacy parameters\n",
        "epsilons = [1.0, 20.0, 100.0, 500.0, math.inf]\n",
        "delta = 1e-08\n",
        "sensitivity = 1e-01\n",
        "\n",
        "# Local learning parameters\n",
        "local_epoch = 20\n",
        "local_lr = 0.001\n",
        "reg_factors = [0.0, 1.0] # IMIA defense mu, 0.0 means no defense\n",
        "\n",
        "metrics = Metrics()\n",
        "\n",
        "# Local training algorithm\n",
        "def train_fncf_functional(model, user_embedding, item_embeddings, interactions, num_epoch, lr, reg_factor):\n",
        "    user_embedding.grad = None\n",
        "    item_embeddings.grad = None\n",
        "    func_model, model_params = functorch.make_functional(model)\n",
        "    opt_params = (user_embedding, item_embeddings, *model_params)\n",
        "    # use_accelerated_op=True would be faster but prevent reconstruction for some reasons. Bug?\n",
        "    # eps_root must be set\n",
        "    optimizer = torchopt.FuncOptimizer(torchopt.adam(lr=lr, eps_root=1e-08))\n",
        "    for _ in range(num_epoch):\n",
        "        preds = func_model(opt_params[2:], opt_params[0], opt_params[1])\n",
        "        reg_loss = reg_factor * F.l1_loss(opt_params[1], item_embeddings) # IMIA defense L1 reg term\n",
        "        loss = F.binary_cross_entropy(preds.view(-1), interactions) + reg_loss\n",
        "        opt_params = optimizer.step(loss, opt_params)\n",
        "    return item_embeddings - opt_params[1], opt_params[2:]\n",
        "\n",
        "# Simulate attack on each user\n",
        "for user_id in tqdm(user_ids):\n",
        "    # Sample items and interactions\n",
        "    interacted_items = data.get_item_ids_for_users([user_id])[0]\n",
        "    non_interacted_items = data.get_non_interacted_item_ids_for_users([user_id])[0]\n",
        "\n",
        "    num_pos = len(interacted_items)\n",
        "    sampled_non_interacted_items = random.sample(\n",
        "        non_interacted_items,\n",
        "        min(num_pos * neg_sample_ratio, len(non_interacted_items)),\n",
        "    )\n",
        "    num_neg = len(sampled_non_interacted_items)\n",
        "    num_data = num_pos + num_neg\n",
        "\n",
        "    user_embedding = (\n",
        "        user_embeddings(torch.LongTensor([user_id_to_idx[user_id]]))\n",
        "        .detach()\n",
        "        .view(-1)\n",
        "    )\n",
        "    item_embedding = item_embeddings(\n",
        "        torch.cat(\n",
        "            [\n",
        "                torch.LongTensor([item_id_to_idx[id] for id in interacted_items]),\n",
        "                torch.LongTensor(\n",
        "                    [item_id_to_idx[id] for id in sampled_non_interacted_items]\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "    ).detach()\n",
        "    user_embedding.requires_grad_()\n",
        "    item_embedding.requires_grad_()\n",
        "    interactions = torch.cat([torch.ones(num_pos), torch.zeros(num_neg)]) # Ground truth interactions\n",
        "    random_user_emb = torch.rand(embedding_dim, requires_grad=True) # The server doesn't know the real user embedding\n",
        "\n",
        "    for epsilon in epsilons:\n",
        "        for reg_factor in reg_factors:\n",
        "            # Local training\n",
        "            target, target_model_params = train_fncf_functional(fncf, user_embedding, item_embedding, interactions, local_epoch, local_lr, reg_factor)\n",
        "            target = apply_gaussian_mechanism(target.detach(), epsilon, delta, sensitivity=sensitivity)\n",
        "\n",
        "            # Attack\n",
        "            mean_norm = torch.linalg.vector_norm(target, dim=1).mean()\n",
        "            norm_scale = max(torch.Tensor([1.0]), torch.Tensor([1e+02]) / mean_norm)\n",
        "            custom_loss = lambda e1, e2: F.pairwise_distance(e1, e2).mean() * norm_scale\n",
        "            preds_raw, _ = reconstruct_interactions(\n",
        "                lambda I: train_fncf_functional(fncf, random_user_emb, item_embedding, I, local_epoch, local_lr, reg_factor)[0] / local_lr,\n",
        "                target / local_lr,\n",
        "                num_data,\n",
        "                lr=atk_lr,\n",
        "                max_iter=max_iter,\n",
        "                num_rounds=num_atk,\n",
        "                loss_fn=custom_loss,\n",
        "                return_raw=True,\n",
        "            )\n",
        "            preds = preds_raw.sigmoid().round().long()\n",
        "\n",
        "            # Update attack performance\n",
        "            metrics.update(\n",
        "                f\"FNCF_eps_{epsilon}_IMIA_{reg_factor}\",\n",
        "                interactions,\n",
        "                preds,\n",
        "                preds_raw=preds_raw,\n",
        "            )\n",
        "\n",
        "metrics.save(\"./rec_metrics.csv\")\n",
        "metrics.print_summary([\"auc\", \"f1\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}